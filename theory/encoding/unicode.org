#+TITLE: Unicode

* Prerequisites

** Endianness

Normally this refers to byte endianness, but information can be stored in small or large groups of binary bits. So endianness might refer to endianess of bits (bit numbering, typically used in low-level transmission protocol), octets, hextets, words, etc. The most common positional notation used in mathematics is digit-big-endian: =123456=, with the most significant digit at the first place.

In computing, endianness refers to the ordering of a sequence of storage cells (usually in octets) that represents a primitive data type that can be manipulated by a single hardware instruction. Typically, the first storage cell would be at the lower address of memory. The relation between "significance" of a storage cell in the data and its order determines the endianness.

- /big-endianness/: decreasing numeric significance with increasing memory addresses (or increasing time)

- /little-endianness/: increasing numeric significance with increasing memory addresses (or increasing time)

Some architectures uses different byte endianness for integers and floating-point numbers. Some even has different endianness conventions for words and bytes inside a double-precision number.

Side note: in C, the index of an array member increases with its pointer
(virtual memory address).

** Writing Systems

- /Alphabetic scripts/

- /Consonant scripts/, /abjads/

- /Abugida scripts/

- /Syllabic scripts/

- /Ideographic scripts/

** Font

An organized set of glyphs.

A font may have the same glyph for different characters or multiple glyphs for
the same character.

Font data may be embedded into a document so that the document can be presented
as designed.

- *typeface*: the basic design of glyphs, the word "font" for particular
  implementations and variants.

*** Classification

- *Serif*

- *Sans Serif*

- *Monospace*, contrast with *proportional*

- *Cursive*

- *Fantasy*

*** Implementations

- Bitmap/raster fonts

- Postscript Type 1

- TrueType =ttf=

- OpenType: unicode oriented and more platform-independent.

** Character Encodings

The process of presenting characters in digital form as sequences of octets.

All these code points make up a /code space/, /code page/ (originally coded in
graphics hardware in IBM PC) or a /character map/.

The word /character set/ should be understood as the set of internal
representation of characters, a.k.a character encoding instead of character repertoire.

- /grapheme/: the smallest functional unit of a writing system in linguistics.
  Not every grapheme is represented in Unicode as a single code point. Some are
  represented by a sequence of two or more Unicode characters.

- /glyph/: a specific shape that represents any particular grapheme in a given
  typeface, a visible manifestation of a character.

- /character/:  a minimal unit of text with semantic value

- /character set/: a collection of characters that might be used by multiple languages

- /coded character set/: a character set in which each character corresponds to a unique number

- /code point/: any allowed value in a coded character set, consisting of one or more code units.

- /code space/: a set of code points

- /code unit/ or /code value/: a bit sequence to encode each character of a repertoire (the abstract set of characters of concern) within a given encoding form.

* [[https://home.unicode.org/][Unicode]]

A standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. Maintained by the Unicode Consortium.

Unicode can be implemented by different character encodings: UTF-8, UTF-16,
UTF-32 and even GB18030.

** Design Principles

- /Universality/: incorporate different languages and writing systems; acting as
  an intermediate code.

- /Efficiency/: ease of character processing with a single mapping from numbers
  to characters; some tradeoff has to be made.

- /Characters, not glyphs/

- /Semantics/: characters have well-defined meanings, mostly the properties of characters.

- /plain text/: Unicode does not deal with formatting or structuring information

- /Logical order/ rather visually: a diacritic is placed after the character it
  modifies even if it's visually on top of the character.

- /unification/: encodes duplicates of a character as a single code point even
  if they might be from different languages. e.g. Han Unification.
  + across glyph variation
  + no unification across scripts
  + unified diacritics
  + across different usages (=.=)
  + category difference may prevent unification.
  + limited by convertibility from/to other character standards since they may
    not have unified these characters.
  + Han unification

- /Dynamic composition/: Characters with diacritic marks can be composed
  dynamically, using characters designated as combining marks.

- /Equivalent sequences/: precomposed forms and their decompositions are seen equivalent.

- /Convertibility/: accurate conversion between Unicode and other character
  standards and specifications.
  + an extension of ISO-8859-1 and therefore also an extension of ASCII (not in
    encoded forms since ISO-8859-1 uses only one octet).

** Abstract Character

A Unicode character is an abstract concept, not defined by a glyph, a name, a
phoneme but a symbol whose various representations are understood to mean the
same thing by a community of people.

- Has no particular stylistic appearance as long as the designs can be
  recognized by the same character.

- Has an official name but no fixed name across languages.

- Has no fixed pronunciation.

- May have very specific usage or a broad range of different uses.

The intuitive concept of character varies by language and cultural background.
Similar characters in different writing systems are not merged and some special
forms in shape are not considered the same character.

A character is ultimately rendered by an image, but an image is not necessarily
encoded into a Unicode character.

Encoding a character is not just about assigning a number to it: it is about
giving it an /identity/. The unicode name of a character is also its unique
identity. Such identities can be extended to traditional text processing so that
a character can be unmistakenly specified.

*** Character Processing Operations

- searching, replacing, sorting, copying, indexing, modifying, computing
  statistics, spelling and grammar checks; automatic translation

- rendering with fonts; bolding, italics and other features; forming ligature;
  adjusting spacing;

** Notation

Uppercase for Unicode names; =U+XXXX= for code points.

** Character definition

- [[https://unicode.org/charts/][Code Charts]]

- Many different characters may look similar, but mixing them may cause issues
  with computerized text processing. Unicode sometimes even uses the same glyph for
  different characters.

- Some characters are defined to be variants of other characters e.g.
  compatibility equivalence.

*** Identity

- /Unicode number/ (code point): a 21-bit integer
  + not necessarily the criteria for comparison

- /Unicode name/

- /Named character sequence/: Some character sequence are defined and named by UCS and Unicode so that composed characters
  may have their individual identity.

** [[https://www.rfc-editor.org/rfc/rfc2130.txt][IAB Model]]

- Coded Character Set

- Character Encoding Scheme

- Transfer Encoding Syntax: e.g. Base64, BinHex, Quoted Printable, uuencode, since the recipient may not be able to
  handle all octets properly (having inappropriate assumptions about the underlying charsets).

** Unicode Model

- Abstract Character Repertoire

- Coded Character Set

- Character Encoding Form

- Character Encoding Scheme

/Character Map/: A mapping of character strings (sequences of abstract
characters) to sequences of octets, ignoring the intermediate levels.

** Coding Space (Codespace)

The range of integers that can be used as character numbers. Although not of
interest to others, the Unicode coding space are structurally organized.

- /code point/: a character number in Unicode coding space.

- /Code plane/: divided into seventeen planes. The first 5-bit specifying the
  plane and the rest inside a plane, with each plane of 0x10000 characters, in
  total 1114112 characters in Unicode. Only three are used and the others are
  currently not assigned.
  + /Basic Multilingual Plane/ (BMP): accessed as a single code unit in UTF-16.
  + the latter planes are encoded in four bytes in UTF-8 and surrogate pairs in
    UTF-16.
  + classification: graphic, format, control, private use, surrogate,
    noncharacter, reserved
  + =U+FEFF= (BOM) and =U+FFFE= (in case of byte reversal, 0xFEFF becomes
    0xFFFE)

- /row/: a set of 256 characters inside a plane. =U+1234= is inside the 12th row
  in the 0th plane.

- /block/: logical/semantic division inside a plane.
  + e.g. Basic Latin (ASCII), Arrows, Mathematical Operators

- /Surrogate/: part of the BMP plane is reserved for surrogates to represent
  characters outside the BMP in UTF-16 so that a surrogate code unit does not conflict with
  a real Unicode character in UTF-16.

** Properties of Characters

Character properties are normative, unlike informative character description,
defined in an /exact/, /objective/ and /formalized/ manner. Character properties
are not guaranteed to be stable. Properties can be /basic/ or /derived/ (from
other properties)

*** Property Values

- /name/

- /type/

- /abbreviation/

- /description of the meaning/

- /status/:
  + normative: the property, if implemented, must be used as specified in the
    standard.
  + informative: used as you like.

*** Common Properties

- /General Category/: a classification of Unicode characters, with a major class
  and a subclass. e.g. Letter, uppercase; Mark, nonspacing; Punctuation,
  connector;

- /Common Exclusion/: the character is excluded from composition in
  normalization due to certain reasons.

*** Composition/Decomposition

Unicode allows an unlimited number of combining diacritic marks combined with a
/base character/.

A precomposed character (/composite/ character, /decomposable/ character) may be
formed visually, defined by Unicode, by combining certain other
characters (/canonical decomposition/), forming a /canonical equivalence/
relationship. In Unicode, such diacritics always follow the base character.

- /decomposition mapping/: associates another character or a sequence of
  characters with the given character.
  + /canonical mapping/: different way to encode the same symbol in Unicode,
    should normally be rendered the same but not forced to.
    - If combing diacritic marks belong to different combining classes, their
      order are fixed by combining class number. However, if they belong to the
      same class, the order of composition does matter.
  + /compatibility mapping/: fundamentally similar characters which may differ
    in rendering as well as in scope of usage and in practice in meaning as
    well. It includes /canonical mapping/.

A character that is canonical equivalent to something other than itself is said
to be /canonical decomposable/ . Similarly, if a character is compatibility
equivalent to something other than itself, it is /compatibility decomposable/.

- /compatibility character/: Compatibility characters were included into
  Unicode for compatibility with other character codes.
  + The overall tone of the standard is that compatibility characters should be
   avoided, except in legacy data or, if unavoidable, in plain text. In formats
    other than plain text, it is often possible and suitable to avoid
    compatibility characters by using markup or other tools.
  + not all compatibility characters have compatibility decompositions, of which
    compatibility decomposition differs from its canonical decomposition.
    e.g. U+212B Å is such while U+00C5 Å is not even a compatibility character.
    However, wider concept of compatibility character is basically just
    descriptive; rules and algorithms operate on the decompositions.

Precomposed ligature are not recommended since most programs cannot properly
treat them as their decomposed sequences. Instead, zero-width joiner and
non-joiner should be used.

*** Normalization

Some forms are defined as /normal forms/ and other forms can be converted to
them. processing of data becomes easier if the variation is reduced.

- /Normalization/: deal with canonical and compatibility decompositions and
  compositions, in contrast with /folding/.

- /Normalization Forms/:
  + NF + D (Canonical Decomposition)/C (canonical Composition)/KD (Compatibility decomposition)/KC
    (compatibility decomposition, canonical composition). Here composition
    implies prior decomposition to ensure a uniform manner of diacritic handling.
  + No normalization form performs any "compatibility composition."
  + Normalization Form C (NFC) is favored as the basic form by W3C.

Strings consisting of Basic Latin characters only are not changed in any
normalization.

**** NFC

- /Starter/: a character of combining class 0

- /Blocked/: a character C is blocked from the starter S if there is another
  character B between them such that B is starter or has a combining class value
  higher than C.

- /Primary composite/: a character is a /primary composite/ if it has a
  canonical mapping not is not explicitly excluded from composition (Common
  Exclusion property).

Procedure:

1. Construct the canonical decomposition of the string with consecutive
   nonspacing marks reordered.

2. Process the result by successively composing each character with the nearest
   preceding starter if not blocked from it.

*** Sorting and Collation

- /Collating order/: the order by which sorting of character data takes place.
  + language-dependent and may vary even within a language, depends on a
    /locale/, defined in a cultural environment: *Common Locale Data Repository*
    (CLDR).
  + Collating order should meet user expectations.
  + the modern approach to collation is based on a layered model, where each
    layer may modify or replace the rules set on lower layers.
    - application-specific; company-specific; locale-specific; rule common to
      many locales by tradition; universal default rules

Unicode itself does not define collation. Using code point order as collating
order is unnatural and impractical.

**** Unicode Collation Algorithm

- Multilevel comparison
  + alphabetic ordering
  + diacritic ordering
  + case ordering: lowercase precedes uppercase

UCA is described as an algorithm that takes as input a string and a *Collation
Element Table*, which contains mapping data for characters, and outputs a sort
key.

- *Collation Element Table*: maps characters (code points) to collation
  elements, a sequence of three or more weights in the order of levels and priority.

*** Text Boundaries

Unicode defines some rules for text boundaries, used mainly for text editing
rather than advanced syntactic analysis.

** Terminology

- /Text element/: a sequence of characters treated as a unit in some processing
  e.g. a word, a token.

- /Unicode string/: a string of Unicode code units.

- /General Category/: a property of code points.
  + Letter, Mark, Number, Punctuation, Symbol, Separator, Other

** Fonts

- /Variation Selectors/: some characters are defined in Unicode to indicate some
  glyph variation in a generic way.

- /ligature/: not to be confused with characters that originate from ligatures;
  handled by fonts. Also, Unicode defines some characters to indicate ligature.

Various font tricks are unnecessary with Unicode and are often risky since they
don't change the identities of these characters.

** Issues

- Han Unification, alternative CJK encoding, Unicode variation sequences.

- Combining characters are often not rendered correctly.

** ISO 10464 (UCS)

A standard set of character originally defined by ISO, later synchronized with
Unicode, currently a subset of Unicode.

** Encodings

UTF-8, UTF-16 and UTF-32 are self-synchronizing/auto-synchronizing: if malformed
data is detected, only one code point needs to be rejected–the start of the
representation of the next code point can be recognized easily.

*** UTF-16

The original Unicode encoding scheme (UCS-2) was not sufficient and UCS-4 was too space-inefficent.

BMP code points can be directly encoded with 2 bytes. Supplementary planes are
encoded as two 1-bite code units called /surrogate pair/ (high-low surrogates), following a certain
algorithm (which requires subtraction, bit-shift and addition).

#+begin_quote
U+10437

1. subtracting 0x10000 leaves 0x0437
2. right shift by 10 and add 0xD800 = 0xD801 (high surrogate represents the
   higher 10 bits)
3. take the lower 10 bits and add 0xDC00 = 0xDC37 (low surrogate represents the
   lower 10 bits)
#+end_quote

This algorithm has a variant that involves more bit concatenation.

#+begin_quote
1. Represent the code number as a 21-bit integer
2. divide this integer into three parts with 5 (u1), 6 (u2) and 10 (u3) bits
   respectively
3. u1 = u1 - 1 and take the lower 4 bits
4. high surrogate = 110110u1u2
5. low surrogate = 110111u3
#+end_quote

$$
U = (H - D800_{16}) \times 400_{16} + (L - DC00_{16}) \times 10000_{16}
$$

*** UTF-8

The one-byte part are the same as US-ASCII.

The multibyte-byte parts starts with a header byte and several continuation bytes with =10= as their markers. There are three types of bytes: leading bytes (starts with several =1= and then a =0=), continuation bytes (starts with =10=) and ascii bytes (starts with a =0=)

#+begin_src python
A # 0_1100001
α # 110_01110 10_110001
中 # 1110_0100 10_111000 10_101101
#+end_src

The octets in UTF-8 are limited to certain ranges.

**** Advantages

- Backward compatibility with ASCII and thus related technology. ASCII-related algorithms can be easily applied to UTF-8 (character searching in UTF-8 is just a word-searching of the ASCII version)

- Fallback and auto-detection: it can efficiently detect some error values and successful in the majority of cases.

- Prefix Code: the leading byte can indicate the length of the encoded character.

- Self Synchronization: easy to find and synchronize with the start of a character byte sequence

- Ignore UTF-16 byte order mark

- No special locale and language settings needed

**** Disadvantages

- Less space efficient than specilized local encodings.

*** UTF-32/UCS-4

Mainly used where the data is a single code units or glyph rather than string of
characters.

UTF-32 does not use surrogate values and they should always be in the range of
=U+0000= to =U+D7FF= and =U+E000= to =U+10FFF=.

*** Byte Order

- Use metadata to indicate the byte order

- the data itself can indicate the byte order (byte order mark =U+FEFF= since
  =U+FFFE= is a "noncharacter", not assigned or used in Unicode).

*** Conversions

Conversions from/to UTF-32 are basically how UTF-8 and UTF-16 algorithms works.
Conversion between UTF-8 and UTF-16 requires an intermediate UTF-32 form.

** Conformance

- not every unicode character is required to be included. However, every
  included character should be interpreted according the standard.
  Canonical-equivalent characters may be interpreted as the same or different.

- if a program does not intend to modify the interpretation of the character
  data, then the data itself must not be modified except by replacing a string
  with a canonical equivalent string or removing noncharacters. It should not
  modify characters it does not understand.

- surrogates, noncharacters, unassigned code points should not be treated as characters.

- a program is supposed to interpret a code unit sequence of a certain encoding
  form according to that encoding scheme and generate a code unit of such a
  encoding form without emitting ill-formed code unit sequences. It shall treat
  ill-formed code unit sequence as an error condition while interpreting. Byte
  order shall be properly handled.

-
