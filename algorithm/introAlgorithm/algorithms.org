#+SETUPFILE: /home/djn/Notes/CSNotes/tools/org-html-themes/setup/theme-readtheorg.setup

* Introduction

** Peak Finding (1D Version)

- Formulation: Suppose there is an array of integers. The problem is to find a peak 
(the numbers that are larger than or equal to their neighbors).

#+INCLUDE: "python-implementation/peak_finder_1d.py" src python

- ~naive_peak_finder_1d~: $\Theta(n)$ 

- ~recursive_peak_finder_1d~: $Theta(\log_{2) n$

** Peak Finding (2D Version)

- Formulation: $a$ is a 2D-peak 
iff $a$ is greater than or equal to all its neighbors (the four cells around it).

#+INCLUDE: "python-implementation/peak_finder_2d.py" src python

- ~greedy_ascent_peak_finder~: worst case $\Theta(nm)$

** Models of Computation (the mathematical analog to a computer)

Algorithm: mathematical abstraction of computer program, computational procedure to solve a problem

Specifies what operations an algorithm is allowed, cost (time, space ...) of each operation, cost of algorithm = sum of operations costs

Some other analogs:

- program <-> algorithm

- programming languages <-> pseudocode

- computer <-> model of computation

*** Some Models 

A way of structuring the thinking about how an algorithm is written, s
imilar to how programming styles structure a program.

**** Random Access Machine

Basically what assembly programming is, ignoring caching. A realistic and powerful model.

Every basic objects is assumed to fit into a word.

+ having random access memory

+ load/store a word from/in memory, do computations in registers in constant time

**** Pointer Model

A more abstract model but simpler. Dynamic memory allocation has been taken cared of already. 
It can be implemented with a RAM and many data structures are built this way.

A field is either a word, which stores a basic object, or a pointer (reference). 
The only operation allowed on pointers is to follow them, which costs constant time.

**** The Python Model

Offers both the above models.

- =list=: random-access array, but appending an element takes nearly constant time
 through table doubling

- Pointer-machine style OOP

- =L.sort()= takes $\Theta(\left| n\right| \log\left|n\right|)$ via comparison sort

- =len(L)=: constant time

** Reading Notes

Chap.1, Chap.3, D.1

*instance of the problem*: the input that satisfies the constraints of the problem

*data structure*: a way to store and organize data in order to facilitate access and modifications.

Algorithims considered as a technology, total system performance depends on choosing
 efficient algorithms as much as on choosing fast hardware. 
Algorithims are at the core of most technologies used in contemporary computers.


*** Pseudocode conventions

1. indentation indicates block structure;

2. The loop counter retains its value after exiting the loop and the counter is beyond the final value. ~to~ incremets the counter and ~downto~ decrements.

3. Variables are local to the given procedure.

4. Compound data are organized into *objects*, which are composed of attributes, ~A.length~. 
A variable representing an array or object represents a pointer/reference to the data representing the array or object. Null pointer is denoted as ~NIL~.

5. Parameters are passed by value in nature.

6. ~and~, ~or~ are short circuiting.

*** Growth of Functions

For large enough inputs, the asymptotic efficiency of algorithms is 
more of concern.

**** Asymptotic Notation

Typically confined to $\mathbb{N}$.

- \Theta-notation: For a given function $g(n)$

$$
\Theta \left(g\left(n\right)\right)=\left\{ f \left(n \right)\colon\exists c_{1}>0,c_{2}>0\text{ and }n_{0}\in\mathbb{N}\text{ s.t. }0\leq c_{1}g\left(n\right)\leq f\left(n\right)\leq c_{2}g\left(n\right)\text{ for all } n\geq n_{0} \right\} 
$$

where $f(n)$ and $g(n)$ are required to be /asymptotically nonnegative/ 
(nonnegative whenever $n$ is sufficient large). $g(n)$ is said to be
 an /asymptotically tight bound/ for $f(n)$.

Conventionally, we write $f(n) = \Theta(g(n))$ instead of $f(n) \in \Theta(g(n))$.

The lower-order terms and the coefficient of the highest-order term
of an asymptotically positive function can be ignored in determining
asymptotically tight bounds.

- The /O/-notation is for an symptotic upper bound, a superset of the
\Theta-notation. When used to bound the worst-case running time,
 it is a bound on the running time of the algorithm on every input.
\Theta-notation bound on the worst-case running time does not imply
 a \Theta bound on the running time on every input.

- The \Omega-notation provides an /asymptotic lower bound/.

e.g. For the insertion sort algorithm, the worst-case running time
is $\Omega(n^{2})$, although a best-case runs in $\Theta(n)$ time. 
It is an abuse to say that the running time of insertion sort is 
$O(n^{2})$, since for a given $n$, the actual running time varies,
depending on the particular input of size. "The running time is 
$O(n^2)$" means there is a function that is $O(n^2)$ s.t. for any 
 value of $n$, the running time on that input is bounded from above
by the value $f(n)$.

- /o/-notation: an upper bound that is not asymptotically tight, 
defined as

$o(g(n)) = \{ f(n):$ for *any* positive constant $c > 0$, there exists
 a constant $n_{0} > 0$ s.t. $0 \leq f(n) < cg(n)$ for all $n \geq n_{0} \}$.

Intutively, in /o/-notation, $f(n)$ becomes insignificant relative 
to $g(n)$ as $n$ approaches infinity, i.e.

$$
\lim_{n\to\infty}\frac{f\left(n\right)}{g\left(g\right)}=0
$$

which is directly from the definition.

- \omega-notation

***** Some other notation abuses

- $=$ means $\in$ for a standalone asymptotic notation

- $=$ in a large formula means that a function $f(n)$ is in that set

e.g. 

$$
2n^{2}+\Theta\left(n\right)=\Theta\left(n^{2}\right)
$$

means that for any function $f(n) \in \Theta(n)$, there is some 
function $g(n) \in \Theta(n^{2})$ s.t. $2n^{2} + f(n) = g(n)$ for all
 $n$.

***** Some Properties of Asymptotic Notation

- Transitivity for all five notations

- Reflexivity for \Theta, /O/ and \Omega.

\begin{aligned}
f\left(n\right) &	= \Theta\left(f\left(n\right)\right) \\
f\left(n\right)	& = O\left(f\left(n\right)\right) \\
f\left(n\right)	& = \Omega\left(f\left(n\right)\right)
\end{aligned}

- Symmetry: $f(n) = \Theta(g(n))$ iff $g(n) = \Theta(f(n))$.

- Transpose symmetry: 
   + $f(n) = O(g(n))$ iff $g(n) = \Omega(f(n))$
   + $f(n) = o(g(n))$ iff $g(n) = \omega(f(n))$

Not all functions are asymptotically comparable.

**** Running times comparison 

- $n^{b}=o\left(a^{n}\right)$, any exponential function with a base greeater
 than $1$ grows faster than any polynomial function.

- $\lg^{b}n=o\left(n^{a}\right)$: any positive polynomial function grows
 faster than any polylogarithmic function

- For factorial:
    + $n!=o\left(n^{n}\right)$
    + $n!=\omega\left(2^{n}\right)$
    + $\lg\left(n!\right)=\Theta\left(n\lg n\right)$

- Function iteration: a function itertively applied $i$ times to an initial 
value of $n$.

$$
f^{\left(i\right)}\left(n\right)=\begin{cases}
n & \text{if }i=0\\
f\left(f^{\left(i-1\right)}\left(n\right)\right) & \text{if }i>0
\end{cases}
$$

#+BEGIN_SRC python
# Run this in sage
n = var('n')

log2n = log(n, 2)
sqt = sqrt(n)
y = n
nlgn = n*log(n, 2)
quad = n^2
cube = n^3
expon = 2^n
factori = factorial(n)


plog2n = plot(log2n, (x, 0, 20), color='red', 
              legend_label='$\log_{2} n$')
psqt = plot(sqt, (x, 0, 20), color='cyan',
           legend_label='$\sqrt{n}$')
py = plot(y, (x, 0, 20), color='black',
         legend_label='$n$')
pnlgn = plot(nlgn, (x, 0, 20), color='blue',
            legend_label='$n \log_{2} n$')
pquad = plot(quad, (x, 0, 20), color='green',
            legend_label='$n^{2}$')
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$',ymax=50)

plog2n + psqt + py + pnlgn + pquad + pcube + pfact + pexpon

pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$', ymax=1200)
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon + pfact + pcube
#+END_SRC


* Sorting and Trees

Input: array =A[1...n]= of numbers

Output: permutation =B[1...n]= of =A= such that 
$B[1] \leq B[2] \leq \cdots \leq B[n]$

** Sorting

*** Insertion Sort

- Pseudocode

#+BEGIN_SRC python
for j = 2 to A.length
    key = A[j] // insert A[j] into the sorted sequence A[1...j-1].
    i = j - 1
    while i > 0 and A[i] > key
        A[i+1] = A[i]
        i = i - 1
    A[i+i] = key  
#+END_SRC

Worst-case running time $\Theta(n^{2})$ for a reversed-ordered array.

An improvement to the naive version of insertion sort is to use binary
 search on the already sorted part of the array, which yields 
$\Theta(n\lg n)$ running time if the swap operation is cheap.
**** Implementations

#+INCLUDE: "python-implementation/insertion_sort.py" src python

#+begin_src cpp
// The boost implementation

template < class Iter_t, typename Compare = compare_iter < Iter_t > >
static void insert_sort (Iter_t first, Iter_t last,
                         Compare comp = Compare())
{
    //--------------------------------------------------------------------
    //                   DEFINITIONS
o    //--------------------------------------------------------------------
    typedef value_iter< Iter_t > value_t;

    if ((last - first) < 2) return;


    for (Iter_t it_examine = first + 1; it_examine != last; ++it_examine)
    {
        value_t Aux = std::move (*it_examine);
        Iter_t it_insertion = it_examine;

        while (it_insertion != first and comp (Aux, *(it_insertion - 1)))
        {
            *it_insertion = std::move (*(it_insertion - 1));
            --it_insertion;
        };
        *it_insertion = std::move (Aux);
    };
};
#+end_src

#+INCLUDE: "./c++-implmentation/chap.2/insertion_sort.cc" src C++

*** Merge Sort

Pseudocode

#+BEGIN_SRC python
def merge-sort(A, p, r):
    if p < r:
        q = (p + r) // 2

        merge-sort(A, p, q)
        merge-sort(A, q+1, r)
        merge(A, p, q, r)

def merge(A, p, q, r):
    n1 = q - p + 1
    n2 = r - q
    # copy for inplace merge
    let L[1..n1 + 1] and R[1..n2 + 1] be new arrays
    for i = 1 to n1:
        L[i] = A[p + i - 1]
    for j = 1 to n2:
        R[j] = A[q + j]
    L[n1 + 1] = inf # sentinel 
    R[n2 + 1] = inf
    i = 1
    j = 1

    # merge starts here
    for k = p to r:
        if L[i] <= R[j]:
            A[k] = L[i]
            i = i + 1
        else:
            A[k] = R[j]
            j = j + 1
#+END_SRC

The =merge= procedure takes time $\Theta(n)$.

The worst case running time is 

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n=1\\
2T\left(n/2\right)+\Theta\left(n\right) & \text{if }n>1
\end{cases}
$$

resolved to $T\left(n\right)=\Theta\left(n\lg n\right)$.

**** Implementation

Python implementation

#+include: ./python-implementation/merge_sort.py src python

Home brewed implementation

#+include: ./c++-implmentation/chap.2/merge_sort.cc src C++

libstdc++ =std::merge= implementation

#+BEGIN_SRC C++
  template<typename _InputIterator1, typename _InputIterator2,
	   typename _OutputIterator, typename _Compare>
    _OutputIterator
    __merge(_InputIterator1 __first1, _InputIterator1 __last1,
	    _InputIterator2 __first2, _InputIterator2 __last2,
	    _OutputIterator __result, _Compare __comp)
    {
      while (__first1 != __last1 && __first2 != __last2)
	{
	  if (__comp(__first2, __first1))
	    {
	      *__result = *__first2;
	      ++__first2;
	    }
	  else
	    {
	      *__result = *__first1;
	      ++__first1;
	    }
	  ++__result;
	}
      return std::copy(__first2, __last2,
		       std::copy(__first1, __last1, __result));
    }  
#+END_SRC

** Reading Notes

1.2, 2.1-2.3, 4.3-4.6;

*** Loop Invariants and The correctness of an algorithm

To show the algorithm is correct, we require that for such a loop, the
 properties of ~A[i...j-1]~ (a loop invariant, here sortedness) must satisfy:

- Initialization: it is true prior to the first iteration of the loop;

- Maintenance: If it is true before an iteration of the loop, it remains
 true before the next iteration;

- Termination: When the loop terminates, the invariant gives us a useful
 property that helps show that the algorithm is correct. (a point where 
mathematical induction should stop)

*** Algorithm Analysis

*analyzing an algorithm*: predicting the resources that the algorithm requires 
such as memory, communication bandwidth, or computer hardware, but most often
computational time.

*running time of an algorithm*: the number of primitive operations or "steps" 
executed. More often, we consider the *rate/order of growth* of the 
running time. We usually consider one algorithm to be more efficient than
 another if its worst-case running time has a lower order of growth.

Even for inputs of a given size, an algorithm's running time may depend on
 which input of that size is given. The best case for insertion sort occurs
if the array is already sorted, resulted in a linear running time. The 
worst case running time is a quadratic function. Normally, we concentrate
on finding only the *worst-case running time* (The average case is often
 rougly as bad as the worst case, as in the case of insertion sort). When
analyzing the average-case running time, the technique of probabilistic 
analysis is applied.

*** Algorithm Designing

- /incremental/ approach: e.g. insertion sort

- /divide-and-conquer/ approach: to solve a given problem, an algorithm
 calls itself recursively one or more times to deal with closely related
subproblems. e.g. merge sort
    + Divide the problem into a number of subproblems that are smaller 
    instances of the same problem
    + conquer the subproblem by solving them recursively (recursive cases).
    If the subproblem sizes are small enough, solve them in a straightforward 
    manner.
    + combine the solutions to the subproblems into the solution for the 
    original problem.

The running time of a recursive call is described by a /recurrence equation/
or /recurrence/, which describes the overall running time on a problem
of size $n$ in terms of the running time on smaller inputs.

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n\leq c\\
aT\left(n/b\right)+D\left(n\right)+C\left(n\right) & \text{otherwise}
\end{cases}
$$

where $c$ is the size small enough for the straightforward solution; 
$a$ is the number of subproblems; $D(n)$  is time to divide the problem
 and $C(n)$ is the time to combine the solutions to the subproblems.

This equation can be solved intuitively sometimes by drawing a 
/recurrion tree/.

*** More about Divide-and-Conquer (Chap.4)

**** Three Methods for solving recurrences

****** Substitution Method
        
1. Guess the form of the solution

2. Use mathematical induction (the strong version is more often used) to find
 the constants and show that the solution works.

The guess is that $T(n) = O(n \lg n)$ for the following recurrence

$$
T\left(n\right)=2T\left(\lfloor n/2\rfloor\right)+n
$$

- Assuming that this bound holds for all positive $m < n$ for an appropriate 
choice of $c$, in particular for $m = \lfloor n / 2\rfloor, yielding 
$T\left(\lfloor n/2\rfloor\right)\leq c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)$
, substituting into the recurrence yields

\begin{aligned}
T\left(n\right)	& \leq2(c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)+n \\
	& \leq cn\lg\left(n/2\right)+n \\
	& =cn\lg n-cn\lg2+n \\
	 & =cn\lg n-cn+n \\
	 & \leq cn\lg n \\
\end{aligned}

- Since we only need to prove $T(n) \leq cn\lg n$ for $n \geq n_{0}$, the 
troublesome boundary condition $T(1) = 1$ is removed form consideration in the
induction.

Guessing a solution takes experience and creativity. Sometimes, a little 
algebraic manipulation can make an unknown recurrence similar to what has been
seen.

$$
T\left(n\right)=2T\left(\lfloor\sqrt{n}\rfloor\right)+\lg n
$$

Renaming $m = \lg n$ yields 

$$
T\left(2^{m}\right)=2T\left(2^{m/2}\right)+m
$$

Renaming $S(m) = T(2^{m})$

$$
S\left(m\right)=2S\left(m/2\right)+m
$$

Then 

$$
T\left(n\right)=T\left(2^{m}\right)=S\left(m\right)=O\left(m\lg m\right)=O\left(\lg n\lg\lg n\right)
$$

****** Recursion-tree Method

A recursion tree is best used to generate a good guess, which can then be 
verified by the substitution method.

It converts the recurrence into a tree whose nodes represent the costs incurred
at various levels of the recursion and uses techniques for bounding summations
to solve the recurrence. Each node represents the cost of a single subproblem
somewhere in the set of recursive function invocations. We sum the costs within
each level of the tree to obtain a set of per-level costs.

Floors and ceilings usually do not matter when solving recurrences, so finding
an upper bound is easier without them.


****** Master Method

Provides bounds for recurrences of the form

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $a \geq 1$, $b > 1$ and $f(n)$ is a given function.

- (Master theorem) Let $a \geq 1$ and $b \ge 1$ be constants, let $f(n)$ be a
function, and let $T(n)$ be defined on the nonnegative integers by the 
recurrence

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $n/b$ is interpreted to be mean either $\lfloor n/b \rfloor$ or 
$\lceil n/b \rceil$. Then $T(n)$ has the following asymptotic bounds

1. If $f\left(n\right)=O\left(n^{\log_{b}a-\epsilon}\right)$ for some constant
 $\epsilon > 0$ then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$.

2. If $f\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$, then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\lg n\right)$

3. If $f\left(n\right)=\Omega\left(n^{\log_{b}a+\epsilon}\right)$ for some  constant $\epsilon > 0$ and if $a f(n/b) \leq cf(n)$ for some constant $c<1$
and all sufficiently large $n$, then $T\left(n\right)=\Theta\left(f\left(n\right)\right)$.

#+TODO: Proof of the master theorem

6.1-6.4

#+TODO: 

10.4, 12.1-12.3

#+TODO: 

13.2, 14

#+TODO: 

8.1-8.3

#+TODO: 

****** 

* Hashing

#+TODO: 

* Numerics

#+TODO: 

* Graphs

#+TODO: 

* Shortest Paths

#+TODO: 

* Dynamic Programming

#+TODO: 

* Advanced Topics

#+TODO: 
