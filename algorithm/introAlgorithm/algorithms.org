#+SETUPFILE: /home/djn/Notes/CSNotes/tools/org-html-themes/setup/theme-readtheorg.setup

* Introduction

** Peak Finding (1D Version)

- Formulation: Suppose there is an array of integers. The problem is to find a peak 
(the numbers that are larger than or equal to their neighbors).

#+INCLUDE: "python-implementation/peak_finder_1d.py" src python

- ~naive_peak_finder_1d~: $\Theta(n)$ 

- ~recursive_peak_finder_1d~: $Theta(\log_{2) n$

** Peak Finding (2D Version)

- Formulation: $a$ is a 2D-peak 
iff $a$ is greater than or equal to all its neighbors (the four cells around it).

#+INCLUDE: "python-implementation/peak_finder_2d.py" src python

- ~greedy_ascent_peak_finder~: worst case $\Theta(nm)$

** Models of Computation (the mathematical analog to a computer)

Algorithm: mathematical abstraction of computer program, computational procedure to solve a problem

Specifies what operations an algorithm is allowed, cost (time, space ...) of each operation, cost of algorithm = sum of operations costs

Some other analogs:

- program <-> algorithm

- programming languages <-> pseudocode

- computer <-> model of computation

*** Some Models 

A way of structuring the thinking about how an algorithm is written, s
imilar to how programming styles structure a program.

**** Random Access Machine

Basically what assembly programming is, ignoring caching. A realistic and powerful model.

Every basic objects is assumed to fit into a word.

+ having random access memory

+ load/store a word from/in memory, do computations in registers in constant time

**** Pointer Model

A more abstract model but simpler. Dynamic memory allocation has been taken cared of already. 
It can be implemented with a RAM and many data structures are built this way.

A field is either a word, which stores a basic object, or a pointer (reference). 
The only operation allowed on pointers is to follow them, which costs constant time.

**** The Python Model

Offers both the above models.

- =list=: random-access array, but appending an element takes nearly constant time
 through table doubling

- Pointer-machine style OOP

- =L.sort()= takes $\Theta(\left| n\right| \log\left|n\right|)$ via comparison sort

- =len(L)=: constant time

** Reading Notes

Chap.1, Chap.3, D.1

*instance of the problem*: the input that satisfies the constraints of the problem

*data structure*: a way to store and organize data in order to facilitate access and modifications.

Algorithims considered as a technology, total system performance depends on choosing
 efficient algorithms as much as on choosing fast hardware. 
Algorithims are at the core of most technologies used in contemporary computers.


*** Pseudocode conventions

1. indentation indicates block structure;

2. The loop counter retains its value after exiting the loop and the counter is beyond the final value. ~to~ incremets the counter and ~downto~ decrements.

3. Variables are local to the given procedure.

4. Compound data are organized into *objects*, which are composed of attributes, ~A.length~. 
A variable representing an array or object represents a pointer/reference to the data representing the array or object. Null pointer is denoted as ~NIL~.

5. Parameters are passed by value in nature.

6. ~and~, ~or~ are short circuiting.

*** Growth of Functions

For large enough inputs, the asymptotic efficiency of algorithms is 
more of concern.

**** Asymptotic Notation

Typically confined to $\mathbb{N}$.

- \Theta-notation: For a given function $g(n)$

$$
\Theta \left(g\left(n\right)\right)=\left\{ f \left(n \right)\colon\exists c_{1}>0,c_{2}>0\text{ and }n_{0}\in\mathbb{N}\text{ s.t. }0\leq c_{1}g\left(n\right)\leq f\left(n\right)\leq c_{2}g\left(n\right)\text{ for all } n\geq n_{0} \right\} 
$$

where $f(n)$ and $g(n)$ are required to be /asymptotically nonnegative/ 
(nonnegative whenever $n$ is sufficient large). $g(n)$ is said to be
 an /asymptotically tight bound/ for $f(n)$.

Conventionally, we write $f(n) = \Theta(g(n))$ instead of $f(n) \in \Theta(g(n))$.

The lower-order terms and the coefficient of the highest-order term
of an asymptotically positive function can be ignored in determining
asymptotically tight bounds.

- The /O/-notation is for an symptotic upper bound, a superset of the
\Theta-notation. When used to bound the worst-case running time,
 it is a bound on the running time of the algorithm on every input.
\Theta-notation bound on the worst-case running time does not imply
 a \Theta bound on the running time on every input.

- The \Omega-notation provides an /asymptotic lower bound/.

e.g. For the insertion sort algorithm, the worst-case running time
is $\Omega(n^{2})$, although a best-case runs in $\Theta(n)$ time. 
It is an abuse to say that the running time of insertion sort is 
$O(n^{2})$, since for a given $n$, the actual running time varies,
depending on the particular input of size. "The running time is 
$O(n^2)$" means there is a function that is $O(n^2)$ s.t. for any 
 value of $n$, the running time on that input is bounded from above
by the value $f(n)$.

- /o/-notation: an upper bound that is not asymptotically tight, 
defined as

$o(g(n)) = \{ f(n):$ for *any* positive constant $c > 0$, there exists
 a constant $n_{0} > 0$ s.t. $0 \leq f(n) < cg(n)$ for all $n \geq n_{0} \}$.

Intutively, in /o/-notation, $f(n)$ becomes insignificant relative 
to $g(n)$ as $n$ approaches infinity, i.e.

$$
\lim_{n\to\infty}\frac{f\left(n\right)}{g\left(g\right)}=0
$$

which is directly from the definition.

- \omega-notation

***** Some other notation abuses

- $=$ means $\in$ for a standalone asymptotic notation

- $=$ in a large formula means that a function $f(n)$ is in that set

e.g. 

$$
2n^{2}+\Theta\left(n\right)=\Theta\left(n^{2}\right)
$$

means that for any function $f(n) \in \Theta(n)$, there is some 
function $g(n) \in \Theta(n^{2})$ s.t. $2n^{2} + f(n) = g(n)$ for all
 $n$.

***** Some Properties of Asymptotic Notation

- Transitivity for all five notations

- Reflexivity for \Theta, /O/ and \Omega.

\begin{aligned}
f\left(n\right) &	= \Theta\left(f\left(n\right)\right) \\
f\left(n\right)	& = O\left(f\left(n\right)\right) \\
f\left(n\right)	& = \Omega\left(f\left(n\right)\right)
\end{aligned}

- Symmetry: $f(n) = \Theta(g(n))$ iff $g(n) = \Theta(f(n))$.

- Transpose symmetry: 
   + $f(n) = O(g(n))$ iff $g(n) = \Omega(f(n))$
   + $f(n) = o(g(n))$ iff $g(n) = \omega(f(n))$

Not all functions are asymptotically comparable.

**** Running times comparison 

- $n^{b}=o\left(a^{n}\right)$, any exponential function with a base greeater
 than $1$ grows faster than any polynomial function.

- $\lg^{b}n=o\left(n^{a}\right)$: any positive polynomial function grows
 faster than any polylogarithmic function

- For factorial:
    + $n!=o\left(n^{n}\right)$
    + $n!=\omega\left(2^{n}\right)$
    + $\lg\left(n!\right)=\Theta\left(n\lg n\right)$

- Function iteration: a function itertively applied $i$ times to an initial 
value of $n$.

$$
f^{\left(i\right)}\left(n\right)=\begin{cases}
n & \text{if }i=0\\
f\left(f^{\left(i-1\right)}\left(n\right)\right) & \text{if }i>0
\end{cases}
$$

#+BEGIN_SRC python
# Run this in sage
n = var('n')

log2n = log(n, 2)
sqt = sqrt(n)
y = n
nlgn = n*log(n, 2)
quad = n^2
cube = n^3
expon = 2^n
factori = factorial(n)


plog2n = plot(log2n, (x, 0, 20), color='red', 
              legend_label='$\log_{2} n$')
psqt = plot(sqt, (x, 0, 20), color='cyan',
           legend_label='$\sqrt{n}$')
py = plot(y, (x, 0, 20), color='black',
         legend_label='$n$')
pnlgn = plot(nlgn, (x, 0, 20), color='blue',
            legend_label='$n \log_{2} n$')
pquad = plot(quad, (x, 0, 20), color='green',
            legend_label='$n^{2}$')
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$',ymax=50)

plog2n + psqt + py + pnlgn + pquad + pcube + pfact + pexpon

pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$', ymax=1200)
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon + pfact + pcube
#+END_SRC

* Data Structures

/Sets/ are as fundamental to computer science as they are to mathematics. The sets
that can grow, shrink or otherwise change over time when manipulated by algorithms
are /dynamic/. Algorithms may require several different types of operations to be
performed on sets.

Each element of a dynamic set is represented by an object whose attributes can 
be examined and manipulatged if a pointer to the object is available. Some 
objects have an identifying /key/. The object may contain /satellite data/.

Operations on a dynamic set can be grouped into two categories, /queries/ and 
/modifying operations/: 
  + =search(S, k)=: return a pointer =x= to an element in S s.t. =x.key = k= or =NIL= if not found
  + =insert(S, x)=: =x= is a pointer to an object
  + =delete(S, x)=
  + =minimum(S)=: returns a pointer to the element of =S= with the smallest key
  + =maximum(S)=
  + =successor(S)=: for a totally ordered set
  + =predecessor(S)=

** Stacks and Queues

Stacks and queues are dynamic sets in which the element removed from the set by
=delete= is prespecified.

*** Stacks

- =insert=/=push=

- =delete=/=pop=

Assuming an array-based stack:

#+BEGIN_SRC 
stack-empty(S)
    if S.top == 0
        return True
    else
        return False

push(S, x):
    S.top = S.top + 1
    S[S.top] = x

pop(S):
    if stack-empty(S)
        error "underflow"
    else
        S.top = S.top - 1
        return S[S.top+1]
#+END_SRC

*** Queues

The queue has a /head/ and a /tail/.

- =insert=/=enqueue=

- =delete=/=dequeue=

An array-based implementation would be:

#+BEGIN_SRC 
# Q.tail: the next location to place an element
# Q.head: the first element of the queue
# [Q.head, Q.tail): when Q.head == Q.tail, the queue is empty
enqueue(Q, x):
    Q[Q.tail] = x
    if Q.tail == Q.length
        Q.tail = 1
    else
        Q.tail = Q.tail + 1

dequeue(Q)
    x = Q[Q.head]
    if Q.head == Q.length
        Q.head = 1
    else
        Q.head = Q.head + 1
    return x
#+END_SRC

*** Linked Lists

A linked list may be singly linked, doubly linked and even circular.

#+BEGIN_SRC 
# Unsorted doubly linked list

list-search(L, k)
    x = L.head
    while x != NIL and x.key != k
          x = x.next
    return x

# splices x ont othe front of the linked list
list-insert(L, x)
    x.next = L.head
    if L.head != NIL
        L.head.prev = x
    L.head = x
    x.prev = NIL

list-delete(L, x)
    if x.prev != NIL
        x.prev.next = x.next
    else
        L.head = x.next
    if x.next != NIL
        x.next.prev = x.prev
#+END_SRC

/sentinel/: a dummy object

With a sentinel as the head and the tail of a circular doubly linked list, the
boundary condition can be ignored since there must be a successor and a predecessor
for any element.

#+BEGIN_SRC 
list-search'(L, k)  
    x = L.nil.next
    while x != L.nil and x.key != k
        x = x.next
    return x

list-insert'(L, x)
    x.next = L.nil.next
    L.nil.next.prev = x
    L.nil.next = x
    x.prev = L.nil

list-delete'(L, x)
    x.prev.next = x.next
    x.next.prev = x.prev
#+END_SRC

Sentinels can reduce constant factors. The gain from using sentinel within loops
is usually a matter of clarity of code. Using sentinels helps to tighten the code
in a loop, thus reducing the coefficients of $n$ or $n^{2}$ in the running time.

*** Implementing pointers and objects

1. We can represent a collection of objects that have the same attributes by using
an array for each attribute. A pointer is an index.

2. In many programming languages, an object occupies a contiguous set of locations
in the computer memory. A pointer is simply the address. 

*** Rooted Trees

/binary trees/: node ={key, parent, left, right}=. If =x.parent == NIL=, then =x=
is the root (has no parent), same for =x.left == NIL= or =x.right == NIL=.

/Rooted trees with unbounded branching/: The /left-child, right-sibling representation/
 is a clever scheme to represent trees with arbitrary numbers of children. 
Using only $O(n)$ space for any n-node rooted tree. Each node has two pointers 
besides =parent=:
1. =x.left-child= points to the leftmost child of node =x=
2. =x.right-sibling= points to the sibling of =x= immediately to its right.

* Sorting and Trees

Input: array =A[1...n]= of numbers

Output: permutation =B[1...n]= of =A= such that 
$B[1] \leq B[2] \leq \cdots \leq B[n]$

A sorting algorithm sorts /in place/ if only a constant number of  elements 
of the input array are ever stored outside the array.

Insertion sort, merge sort, heap sort and quick sort are all comparison sorts:
they determine the sorted order of the input array by comparing elements. The 
decision tree model shows the performance limitation of comparison sorts.

The lower bound of $\Omega(n\lg n)$ can be beaten if more information about the
sorted order of the input can be gathered by means other than comparing elements.
e.g. counting sort, radix sort, bucket sort.

| Algorithm       | Worst-case running time     | Average-case/expected running time     | 
|-----------------+-----------------------------+----------------------------------------+
| Insertiion sort | $\Theta(n^{2})$             | $\Theta(n^{2})$                        | 
| Merge Sort      | $\Theta\left(n\lg n\right)$ | $\Theta\left(n\lg n\right)$            | 
| Heapsort        | $O\left(n\lg n\right)$      | -                                      | 
| Quicksort       | $\Theta\left(n^{2}\right)$  | $\Theta\left(n\lg n\right)$ (expected) | 
| Counting sort   | $\Theta(k+n)$               | $\Theta(k+n)$                          | 
| Radix Sort      | $\Theta(d(n+k))$            | $\Theta(d(n+k))$                       | 
| Bucket sort     | $\Theta(n^{2})$             | $\Theta(n)$ (average case)             | 


** Sorting

*** Insertion Sort

A fast in-place sorting algorithm for small input sizes

- Pseudocode

#+BEGIN_SRC python
for j = 2 to A.length
    key = A[j] // insert A[j] into the sorted sequence A[1...j-1].
    i = j - 1
    while i > 0 and A[i] > key
        A[i+1] = A[i]
        i = i - 1
    A[i+i] = key  
#+END_SRC

Worst-case running time $\Theta(n^{2})$ for a reversed-ordered array.

An improvement to the naive version of insertion sort is to use binary
 search on the already sorted part of the array, which yields 
$\Theta(n\lg n)$ running time if the swap operation is cheap.
**** Implementations

#+INCLUDE: "python-implementation/insertion_sort.py" src python

#+begin_src cpp
// The boost implementation

template < class Iter_t, typename Compare = compare_iter < Iter_t > >
static void insert_sort (Iter_t first, Iter_t last,
                         Compare comp = Compare())
{
    //--------------------------------------------------------------------
    //                   DEFINITIONS
o    //--------------------------------------------------------------------
    typedef value_iter< Iter_t > value_t;

    if ((last - first) < 2) return;


    for (Iter_t it_examine = first + 1; it_examine != last; ++it_examine)
    {
        value_t Aux = std::move (*it_examine);
        Iter_t it_insertion = it_examine;

        while (it_insertion != first and comp (Aux, *(it_insertion - 1)))
        {
            *it_insertion = std::move (*(it_insertion - 1));
            --it_insertion;
        };
        *it_insertion = std::move (Aux);
    };
};
#+end_src

#+INCLUDE: "./c++-implmentation/sort/insertion_sort.cc" src C++

*** Merge Sort

The =merge= procedure it uses does not operate in place.

Pseudocode

#+BEGIN_SRC python
def merge-sort(A, p, r):
    if p < r:
        q = (p + r) // 2

        merge-sort(A, p, q)
        merge-sort(A, q+1, r)
        merge(A, p, q, r)

def merge(A, p, q, r):
    n1 = q - p + 1
    n2 = r - q
    # copy for inplace merge
    let L[1..n1 + 1] and R[1..n2 + 1] be new arrays
    for i = 1 to n1:
        L[i] = A[p + i - 1]
    for j = 1 to n2:
        R[j] = A[q + j]
    L[n1 + 1] = inf # sentinel 
    R[n2 + 1] = inf
    i = 1
    j = 1

    # merge starts here
    for k = p to r:
        if L[i] <= R[j]:
            A[k] = L[i]
            i = i + 1
        else:
            A[k] = R[j]
            j = j + 1
#+END_SRC

The =merge= procedure takes time $\Theta(n)$.

The worst case running time is 

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n=1\\
2T\left(n/2\right)+\Theta\left(n\right) & \text{if }n>1
\end{cases}
$$

resolved to $T\left(n\right)=\Theta\left(n\lg n\right)$.

**** Implementation

Python implementation

#+include: ./python-implementation/merge_sort.py src python

Home brewed implementation

#+include: ./c++-implmentation/sort/merge_sort.cc src C++

libstdc++ =std::merge= implementation

#+BEGIN_SRC C++
  template<typename _InputIterator1, typename _InputIterator2,
	   typename _OutputIterator, typename _Compare>
    _OutputIterator
    __merge(_InputIterator1 __first1, _InputIterator1 __last1,
	    _InputIterator2 __first2, _InputIterator2 __last2,
	    _OutputIterator __result, _Compare __comp)
    {
      while (__first1 != __last1 && __first2 != __last2)
	{
	  if (__comp(__first2, __first1))
	    {
	      *__result = *__first2;
	      ++__first2;
	    }
	  else
	    {
	      *__result = *__first1;
	      ++__first1;
	    }
	  ++__result;
	}
      return std::copy(__first2, __last2,
		       std::copy(__first1, __last1, __result));
    }  
#+END_SRC

*** Heap Sort

*priority queue*: a data structure implementing a set S of elements, each associated with a key, supporting the following operations
  + =insert(S, x)=
  + =max(S)=
  + =extract_max(S)=: return the element with the largest key and remove it
  + =update_key(S, x, k)=: update the value of element x's key to new value k

*heap*: an array visualized as a nearly complete binary tree, that implements a
priority queue. The height of a binary heap (the longest simple downward path
from the root to a leaf) is $\lfloor \lg n \rfloor$ and at most $\lceil n/2^{h+1}\rceil$
nodes of any height $h$.
  
=heapify=: correct A[i], which violates the heap property, 
$O(\log n)$ from $T\left(n\right)\leq T\left(2n/3\right)+\Theta\left(1\right)$ (master theorem).

#+BEGIN_SRC python
def max_heapify(A, i):
    l = left(i)
    r = right(i)
    if l <= heap_size(A) and A[l] > A[i]:
          largest = l
    else:
          largest = i
    if r <= heap_size(A) and A[r] > A[largest]:
          largest = r
    if largest != r:
          swap(&A[i], &A[largest])
          max_heapify(A, largest)
#+END_SRC

=build_heap=: converts =A[1...n]= to a heap, $O(n)$. In general, =max_heapify=
takes $O(l)$ for the nodes that are $l$ levels above the leaves. The loop variant
is: at the start of each iteration, each node $i+1$, $i+2$, ..., n is the root of
a heap (which is exactly the preconfition for =heapify=). 

#+BEGIN_SRC python
def build_max_heap(A):
    for i = n // 2 downto 1:    # n//2 is the last non-leaf node
        do max_heapify(A, i)
#+END_SRC


- *min/max heap property*: the key of a node is greater/less than the keys of 
its children. The /root/ of the tree is the /first/ element (i=1) in the array,

#+begin_example
# one-based index  zero-based index
parent(i) = i/2   # (i-1)/2 
left(i) = 2i      # 2i+1
right(i) = 2i+1   # 2i+2
#+end_example

- *heap sort*: every iteration involves a =swap= and a =heapify= operation, hence takes $O(n\log n)$ overall.
  1. build a heap from unordered array
  2.  find maximum element A[1]
  3. swap elements A[n] and A[1], the max element is now at the end of the array
  4. discard node =n= form heap (by decrementing heap size)
  5. new root may violate heap property, but its children are heaps, run =heapify= to fix it.
  6. go to step 2 unless heap is empty

#+BEGIN_SRC 
heap_sort(A):
    build_heap(A)
    for i = A.len downto 2:
        exchange A[i] with A[i]
        A.heap_size = A.heap_size - 1
        heapify(A, 1)
#+END_SRC


**** Implementation

#+include: "./python-implementation/heap_sort.py" src python

*** BST Sort



*** Quicksort

inplace; $\Theta(n^{2})$ worst case, $\Theta(n\lg n)$ expected running time; the
constant factors are small; divide-and-conquer

1. *divide*: partition the array into two (possibly empty) subarrays =A[p..q-1]= and A[q+1..r] such that each element of =A[p..q-1]= is less than or equal to A[q], which is, in turn, less than or equal to each element of =A[q+1..r]=. Compute the index =q= as part of this partitioning procedure

2. *conquer*: sort the two subarrrays =A[p..q-1]= and =A[q+1..r]= by recursive calls

3. *combine*: all subarrays are already sorted, no work for combining them.

#+BEGIN_SRC 
quicksort(A, p, r)
  if p < r
    q = partition(A, p, r)
    quicksort(A, p, q-1)
    quicksort(A, q+1, r)
#+END_SRC

The key to the algorithm is the =partition= procedure, which rearranges the subarray =A[p..r]= in place:

#+BEGIN_SRC 
partition(A, p, r)
  x = A[r]
  i = p - 1     # the last element of the first half subarray
  for j = p to r-1  # the boundary of the rearranged part of the array
    if A[j] <= x
      i = i + 1
      exchange A[i] with A[j] # place the smaller one to the left
  exchange A[i+1] with A[r] # so that the pivot divides the two subarrays
  return i+1  # index of the pivot
#+END_SRC

There are four regions maintained by this procedure:

#+BEGIN_SRC 
| <= pivot | > pivot | unrestricted | pivot |  
#+END_SRC


The loop invariant is: At the beginning of each iteration of the loop, for any
array index $k$:
  1. If $p\leq k\leq i$, then $A[k] \leq x$.
  2. If $i+1 \leq k\leq j-1$ then $A[k] > x$
  3. If $k = r$, then $A[k] = x$

The running time of =partition= on =A[p..r]= is $\Theta(n)$ where $n=r-p+1$.

*** Some other sorting algorithms

**** Selection Sort

inplace comparison sort; $O(n^{2})$ comparison, $O(n)$ swaps (worst-case and 
average-case); generally performs worse than insertion sort; 

The array is divided into a sorted subarray and an unsorted subarray. The algorithm
proceeds by finding the smallest element in the unsorted subarray, exchanging it 
with the leftmost unsorted element and moving the subarray boundaries one element 
to the right.

#+include: "./python-implementation/selection_sort.py" src python

**** Bubble/Sinking Sort

inplace; $O(n^{2})$ comparisons and swaps (worst-case and average-case); primarily
used as an education tool

The algorithm steps through the list, compares adjacent elements and swaps them
if they are in the wrong order.

#+include: "./python-implementation/bubble_sort.py" src python

** Reading Notes

1.2, 2.1-2.3, 4.3-4.6;

*** Loop Invariants and The correctness of an algorithm

To show the algorithm is correct, we require that for such a loop, the
 properties of ~A[i...j-1]~ (a loop invariant, here sortedness) must satisfy:

- Initialization: it is true prior to the first iteration of the loop;

- Maintenance: If it is true before an iteration of the loop, it remains
 true before the next iteration;

- Termination: When the loop terminates, the invariant gives us a useful
 property that helps show that the algorithm is correct. (a point where 
mathematical induction should stop)

*** Algorithm Analysis

*analyzing an algorithm*: predicting the resources that the algorithm requires 
such as memory, communication bandwidth, or computer hardware, but most often
computational time.

*running time of an algorithm*: the number of primitive operations or "steps" 
executed. More often, we consider the *rate/order of growth* of the 
running time. We usually consider one algorithm to be more efficient than
 another if its worst-case running time has a lower order of growth.

Even for inputs of a given size, an algorithm's running time may depend on
 which input of that size is given. The best case for insertion sort occurs
if the array is already sorted, resulted in a linear running time. The 
worst case running time is a quadratic function. Normally, we concentrate
on finding only the *worst-case running time* (The average case is often
 rougly as bad as the worst case, as in the case of insertion sort). When
analyzing the average-case running time, the technique of probabilistic 
analysis is applied.

*** Algorithm Designing

- /incremental/ approach: e.g. insertion sort

- /divide-and-conquer/ approach: to solve a given problem, an algorithm
 calls itself recursively one or more times to deal with closely related
subproblems. e.g. merge sort
    + Divide the problem into a number of subproblems that are smaller 
    instances of the same problem
    + conquer the subproblem by solving them recursively (recursive cases).
    If the subproblem sizes are small enough, solve them in a straightforward 
    manner.
    + combine the solutions to the subproblems into the solution for the 
    original problem.

The running time of a recursive call is described by a /recurrence equation/
or /recurrence/, which describes the overall running time on a problem
of size $n$ in terms of the running time on smaller inputs.

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n\leq c\\
aT\left(n/b\right)+D\left(n\right)+C\left(n\right) & \text{otherwise}
\end{cases}
$$

where $c$ is the size small enough for the straightforward solution; 
$a$ is the number of subproblems; $D(n)$  is time to divide the problem
 and $C(n)$ is the time to combine the solutions to the subproblems.

This equation can be solved intuitively sometimes by drawing a 
/recurrion tree/.

*** More about Divide-and-Conquer (Chap.4)

**** Three Methods for solving recurrences

****** Substitution Method
        
1. Guess the form of the solution

2. Use mathematical induction (the strong version is more often used) to find
 the constants and show that the solution works.

The guess is that $T(n) = O(n \lg n)$ for the following recurrence

$$
T\left(n\right)=2T\left(\lfloor n/2\rfloor\right)+n
$$

- Assuming that this bound holds for all positive $m < n$ for an appropriate 
choice of $c$, in particular for $m = \lfloor n / 2\rfloor, yielding 
$T\left(\lfloor n/2\rfloor\right)\leq c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)$
, substituting into the recurrence yields

\begin{aligned}
T\left(n\right)	& \leq2(c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)+n \\
	& \leq cn\lg\left(n/2\right)+n \\
	& =cn\lg n-cn\lg2+n \\
	 & =cn\lg n-cn+n \\
	 & \leq cn\lg n \\
\end{aligned}

- Since we only need to prove $T(n) \leq cn\lg n$ for $n \geq n_{0}$, the 
troublesome boundary condition $T(1) = 1$ is removed form consideration in the
induction.

Guessing a solution takes experience and creativity. Sometimes, a little 
algebraic manipulation can make an unknown recurrence similar to what has been
seen.

$$
T\left(n\right)=2T\left(\lfloor\sqrt{n}\rfloor\right)+\lg n
$$

Renaming $m = \lg n$ yields 

$$
T\left(2^{m}\right)=2T\left(2^{m/2}\right)+m
$$

Renaming $S(m) = T(2^{m})$

$$
S\left(m\right)=2S\left(m/2\right)+m
$$

Then 

$$
T\left(n\right)=T\left(2^{m}\right)=S\left(m\right)=O\left(m\lg m\right)=O\left(\lg n\lg\lg n\right)
$$

****** Recursion-tree Method

A recursion tree is best used to generate a good guess, which can then be 
verified by the substitution method.

It converts the recurrence into a tree whose nodes represent the costs incurred
at various levels of the recursion and uses techniques for bounding summations
to solve the recurrence. Each node represents the cost of a single subproblem
somewhere in the set of recursive function invocations. We sum the costs within
each level of the tree to obtain a set of per-level costs.

Floors and ceilings usually do not matter when solving recurrences, so finding
an upper bound is easier without them.


****** Master Method

Provides bounds for recurrences of the form

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $a \geq 1$, $b > 1$ and $f(n)$ is a given function.

- (Master theorem) Let $a \geq 1$ and $b \ge 1$ be constants, let $f(n)$ be a
function, and let $T(n)$ be defined on the nonnegative integers by the 
recurrence

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $n/b$ is interpreted to be mean either $\lfloor n/b \rfloor$ or 
$\lceil n/b \rceil$. Then $T(n)$ has the following asymptotic bounds

1. If $f\left(n\right)=O\left(n^{\log_{b}a-\epsilon}\right)$ for some constant
 $\epsilon > 0$ then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$.

2. If $f\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$, then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\lg n\right)$

3. If $f\left(n\right)=\Omega\left(n^{\log_{b}a+\epsilon}\right)$ for some  constant $\epsilon > 0$ and if $a f(n/b) \leq cf(n)$ for some constant $c<1$
and all sufficiently large $n$, then $T\left(n\right)=\Theta\left(f\left(n\right)\right)$.

#+TODO: Proof of the master theorem

6.1-6.4

***** The structure of the data

Typically, the data that are sorted are usually part of a collection of data 
called a /record/. Each record contains a /key/, which is the value to be sorted.
The remainder of the record consists of /satellite data/, which are usually 
carried around with the key.

***** Order Statistics

The $i$-th order statistic of a set of $n$ numbers is the $i$-th smallest number
in the set. We can find this by sorting in $\Omega(n\lg n)$. More efficient 
algorithms are available.



#+TODO: 

10.4, 12.1-12.3

#+TODO: 

13.2, 14

#+TODO: 

8.1-8.3

#+TODO: 

****** 

* Hashing

#+TODO: 

* Numerics

#+TODO: 

* Graphs

#+TODO: 

* Shortest Paths

#+TODO: 

* Dynamic Programming

#+TODO: 

* Advanced Topics

#+TODO: 
