#+SETUPFILE: /home/djn/Notes/CSNotes/tools/org-html-themes/setup/theme-readtheorg.setup

* Introduction

** Peak Finding (1D Version)

- Formulation: Suppose there is an array of integers. The problem is to find a peak 
(the numbers that are larger than or equal to their neighbors).

#+INCLUDE: "python-implementation/peak_finder_1d.py" src python

- ~naive_peak_finder_1d~: $\Theta(n)$ 

- ~recursive_peak_finder_1d~: $Theta(\log_{2) n$

** Peak Finding (2D Version)

- Formulation: $a$ is a 2D-peak 
iff $a$ is greater than or equal to all its neighbors (the four cells around it).

#+INCLUDE: "python-implementation/peak_finder_2d.py" src python

- ~greedy_ascent_peak_finder~: worst case $\Theta(nm)$

** Models of Computation (the mathematical analog to a computer)

Algorithm: mathematical abstraction of computer program, computational procedure to solve a problem

Specifies what operations an algorithm is allowed, cost (time, space ...) of each operation, cost of algorithm = sum of operations costs

Some other analogs:

- program <-> algorithm

- programming languages <-> pseudocode

- computer <-> model of computation

*** Some Models 

A way of structuring the thinking about how an algorithm is written, s
imilar to how programming styles structure a program.

**** Random Access Machine

Basically what assembly programming is, ignoring caching. A realistic and powerful model.

Every basic objects is assumed to fit into a word.

+ having random access memory

+ load/store a word from/in memory, do computations in registers in constant time

**** Pointer Model

A more abstract model but simpler. Dynamic memory allocation has been taken cared of already. 
It can be implemented with a RAM and many data structures are built this way.

A field is either a word, which stores a basic object, or a pointer (reference). 
The only operation allowed on pointers is to follow them, which costs constant time.

**** The Python Model

Offers both the above models.

- =list=: random-access array, but appending an element takes nearly constant time
 through table doubling

- Pointer-machine style OOP

- =L.sort()= takes $\Theta(\left| n\right| \log\left|n\right|)$ via comparison sort

- =len(L)=: constant time

** Reading Notes

Chap.1, Chap.3, D.1

*instance of the problem*: the input that satisfies the constraints of the problem

*data structure*: a way to store and organize data in order to facilitate access and modifications.

Algorithims considered as a technology, total system performance depends on choosing
 efficient algorithms as much as on choosing fast hardware. 
Algorithims are at the core of most technologies used in contemporary computers.


*** Pseudocode conventions

1. indentation indicates block structure;

2. The loop counter retains its value after exiting the loop and the counter is beyond the final value. ~to~ incremets the counter and ~downto~ decrements.

3. Variables are local to the given procedure.

4. Compound data are organized into *objects*, which are composed of attributes, ~A.length~. 
A variable representing an array or object represents a pointer/reference to the data representing the array or object. Null pointer is denoted as ~NIL~.

5. Parameters are passed by value in nature.

6. ~and~, ~or~ are short circuiting.

*** Growth of Functions

For large enough inputs, the asymptotic efficiency of algorithms is 
more of concern.

**** Asymptotic Notation

Typically confined to $\mathbb{N}$.

- \Theta-notation: For a given function $g(n)$

$$
\Theta \left(g\left(n\right)\right)=\left\{ f \left(n \right)\colon\exists c_{1}>0,c_{2}>0\text{ and }n_{0}\in\mathbb{N}\text{ s.t. }0\leq c_{1}g\left(n\right)\leq f\left(n\right)\leq c_{2}g\left(n\right)\text{ for all } n\geq n_{0} \right\} 
$$

where $f(n)$ and $g(n)$ are required to be /asymptotically nonnegative/ 
(nonnegative whenever $n$ is sufficient large). $g(n)$ is said to be
 an /asymptotically tight bound/ for $f(n)$.

Conventionally, we write $f(n) = \Theta(g(n))$ instead of $f(n) \in \Theta(g(n))$.

The lower-order terms and the coefficient of the highest-order term
of an asymptotically positive function can be ignored in determining
asymptotically tight bounds.

- The /O/-notation is for an symptotic upper bound, a superset of the
\Theta-notation. When used to bound the worst-case running time,
 it is a bound on the running time of the algorithm on every input.
\Theta-notation bound on the worst-case running time does not imply
 a \Theta bound on the running time on every input.

- The \Omega-notation provides an /asymptotic lower bound/.

e.g. For the insertion sort algorithm, the worst-case running time
is $\Omega(n^{2})$, although a best-case runs in $\Theta(n)$ time. 
It is an abuse to say that the running time of insertion sort is 
$O(n^{2})$, since for a given $n$, the actual running time varies,
depending on the particular input of size. "The running time is 
$O(n^2)$" means there is a function that is $O(n^2)$ s.t. for any 
 value of $n$, the running time on that input is bounded from above
by the value $f(n)$.

- /o/-notation: an upper bound that is not asymptotically tight, 
defined as

$o(g(n)) = \{ f(n):$ for *any* positive constant $c > 0$, there exists
 a constant $n_{0} > 0$ s.t. $0 \leq f(n) < cg(n)$ for all $n \geq n_{0} \}$.

Intutively, in /o/-notation, $f(n)$ becomes insignificant relative 
to $g(n)$ as $n$ approaches infinity, i.e.

$$
\lim_{n\to\infty}\frac{f\left(n\right)}{g\left(g\right)}=0
$$

which is directly from the definition.

- \omega-notation

***** Some other notation abuses

- $=$ means $\in$ for a standalone asymptotic notation

- $=$ in a large formula means that a function $f(n)$ is in that set

e.g. 

$$
2n^{2}+\Theta\left(n\right)=\Theta\left(n^{2}\right)
$$

means that for any function $f(n) \in \Theta(n)$, there is some 
function $g(n) \in \Theta(n^{2})$ s.t. $2n^{2} + f(n) = g(n)$ for all
 $n$.

***** Some Properties of Asymptotic Notation

- Transitivity for all five notations

- Reflexivity for \Theta, /O/ and \Omega.

\begin{aligned}
f\left(n\right) &	= \Theta\left(f\left(n\right)\right) \\
f\left(n\right)	& = O\left(f\left(n\right)\right) \\
f\left(n\right)	& = \Omega\left(f\left(n\right)\right)
\end{aligned}

- Symmetry: $f(n) = \Theta(g(n))$ iff $g(n) = \Theta(f(n))$.

- Transpose symmetry: 
   + $f(n) = O(g(n))$ iff $g(n) = \Omega(f(n))$
   + $f(n) = o(g(n))$ iff $g(n) = \omega(f(n))$

Not all functions are asymptotically comparable.

**** Running times comparison 

- $n^{b}=o\left(a^{n}\right)$, any exponential function with a base greeater
 than $1$ grows faster than any polynomial function.

- $\lg^{b}n=o\left(n^{a}\right)$: any positive polynomial function grows
 faster than any polylogarithmic function

- For factorial:
    + $n!=o\left(n^{n}\right)$
    + $n!=\omega\left(2^{n}\right)$
    + $\lg\left(n!\right)=\Theta\left(n\lg n\right)$

- Function iteration: a function itertively applied $i$ times to an initial 
value of $n$.

$$
f^{\left(i\right)}\left(n\right)=\begin{cases}
n & \text{if }i=0\\
f\left(f^{\left(i-1\right)}\left(n\right)\right) & \text{if }i>0
\end{cases}
$$

#+BEGIN_SRC python
# Run this in sage
n = var('n')

log2n = log(n, 2)
sqt = sqrt(n)
y = n
nlgn = n*log(n, 2)
quad = n^2
cube = n^3
expon = 2^n
factori = factorial(n)


plog2n = plot(log2n, (x, 0, 20), color='red', 
              legend_label='$\log_{2} n$')
psqt = plot(sqt, (x, 0, 20), color='cyan',
           legend_label='$\sqrt{n}$')
py = plot(y, (x, 0, 20), color='black',
         legend_label='$n$')
pnlgn = plot(nlgn, (x, 0, 20), color='blue',
            legend_label='$n \log_{2} n$')
pquad = plot(quad, (x, 0, 20), color='green',
            legend_label='$n^{2}$')
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$',ymax=50)

plog2n + psqt + py + pnlgn + pquad + pcube + pfact + pexpon

pexpon = plot(expon, (x, 0, 20), color='purple',
             legend_label='$2^{n}$')
pfact = list_plot([factori(n) for n in range(20)], color='orange',
            legend_label='$n!$', ymax=1200)
pcube = plot(cube, (x, 0, 20), color='yellow',
            legend_label='$n^{3}$', xmax=20)
pexpon + pfact + pcube
#+END_SRC

* Data Structures

/Sets/ are as fundamental to computer science as they are to mathematics. The sets
that can grow, shrink or otherwise change over time when manipulated by algorithms
are /dynamic/. Algorithms may require several different types of operations to be
performed on sets.

Each element of a dynamic set is represented by an object whose attributes can 
be examined and manipulatged if a pointer to the object is available. Some 
objects have an identifying /key/. The object may contain /satellite data/.

Operations on a dynamic set can be grouped into two categories, /queries/ and 
/modifying operations/: 
  + =search(S, k)=: return a pointer =x= to an element in S s.t. =x.key = k= or =NIL= if not found
  + =insert(S, x)=: =x= is a pointer to an object
  + =delete(S, x)=
  + =minimum(S)=: returns a pointer to the element of =S= with the smallest key
  + =maximum(S)=
  + =successor(S)=: for a totally ordered set
  + =predecessor(S)=

** Stacks and Queues

Stacks and queues are dynamic sets in which the element removed from the set by
=delete= is prespecified.

*** Stacks

- =insert=/=push=

- =delete=/=pop=

Assuming an array-based stack:

#+BEGIN_SRC 
stack-empty(S)
    if S.top == 0
        return True
    else
        return False

push(S, x):
    S.top = S.top + 1
    S[S.top] = x

pop(S):
    if stack-empty(S)
        error "underflow"
    else
        S.top = S.top - 1
        return S[S.top+1]
#+END_SRC

*** Queues

The queue has a /head/ and a /tail/.

- =insert=/=enqueue=

- =delete=/=dequeue=

An array-based implementation would be:

#+BEGIN_SRC 
# Q.tail: the next location to place an element
# Q.head: the first element of the queue
# [Q.head, Q.tail): when Q.head == Q.tail, the queue is empty
enqueue(Q, x):
    Q[Q.tail] = x
    if Q.tail == Q.length
        Q.tail = 1
    else
        Q.tail = Q.tail + 1

dequeue(Q)
    x = Q[Q.head]
    if Q.head == Q.length
        Q.head = 1
    else
        Q.head = Q.head + 1
    return x
#+END_SRC

** Linked Lists

A linked list may be singly linked, doubly linked and even circular.

#+BEGIN_SRC 
# Unsorted doubly linked list

list-search(L, k)
    x = L.head
    while x != NIL and x.key != k
          x = x.next
    return x

# splices x ont othe front of the linked list
list-insert(L, x)
    x.next = L.head
    if L.head != NIL
        L.head.prev = x
    L.head = x
    x.prev = NIL

list-delete(L, x)
    if x.prev != NIL
        x.prev.next = x.next
    else
        L.head = x.next
    if x.next != NIL
        x.next.prev = x.prev
#+END_SRC

/sentinel/: a dummy object

With a sentinel as the head and the tail of a circular doubly linked list, the
boundary condition can be ignored since there must be a successor and a predecessor
for any element.

#+BEGIN_SRC 
list-search'(L, k)  
    x = L.nil.next
    while x != L.nil and x.key != k
        x = x.next
    return x

list-insert'(L, x)
    x.next = L.nil.next
    L.nil.next.prev = x
    L.nil.next = x
    x.prev = L.nil

list-delete'(L, x)
    x.prev.next = x.next
    x.next.prev = x.prev
#+END_SRC

Sentinels can reduce constant factors. The gain from using sentinel within loops
is usually a matter of clarity of code. Using sentinels helps to tighten the code
in a loop, thus reducing the coefficients of $n$ or $n^{2}$ in the running time.

*** Implementing pointers and objects

1. We can represent a collection of objects that have the same attributes by using
an array for each attribute. A pointer is an index.

2. In many programming languages, an object occupies a contiguous set of locations
in the computer memory. A pointer is simply the address. 

** Rooted Trees

/binary trees/: node ={key, parent, left, right}=. If =x.parent == NIL=, then =x=
is the root (has no parent), same for =x.left == NIL= or =x.right == NIL=.

/Rooted trees with unbounded branching/: The /left-child, right-sibling representation/
 is a clever scheme to represent trees with arbitrary numbers of children. 
Using only $O(n)$ space for any n-node rooted tree. Each node has two pointers 
besides =parent=:
1. =x.left-child= points to the leftmost child of node =x=
2. =x.right-sibling= points to the sibling of =x= immediately to its right.

*** Binary Search Trees

Basic operations on a binary search tree take time proportional to the height 
of the tree. Basic dynamic-set operations on such a tree take $\Theta(\lg n)$ 
time on average.

- /binary-search-tree/ property: Let $x$ be a node in a binary search tree. If 
  $y$ is a node in the left subtree of $x$, then $y.key \leq x.key$. If $y$ is a
  node in the right subtree of $x$, then $y.key \geq x.key$.

#+BEGIN_SRC 
inorder-tree-walk(x):
  if x != NIL:
      inorder-tree-walk(x.left)
      print x.key
      inorder-tree-walk(x.right)
#+END_SRC

Theorem: If $x$ is the root of an $n$ -node subtree, then the call =inorder-tree-walk(x)=
takes $\Theta(n)$ time.

Proof: Since =inorder-tree-walk= visits all $n$ nodes of the subtree, 
$T(n) = \Omega(n)$. Suppose $T(n) \leq (c+d)n +c$, by induction

\begin{aligned}
T\left(n\right) &	\leq T\left(k\right)+T\left(n-k-1\right)+d \\
&	=\left(\left(c+d\right)k+c\right)+\left(\left(c+d\right)\left(n-k-1\right)+c\right)+d \\
&	=\left(c+d\right)n+c-\left(c+d\right)+c+d \\
&	=\left(c+d\right)n+c \\
& = O(n)
\end{aligned}

where $d > 0$  is upper bound on the time to execute the body of =inorder-tree-walk(x)=,

- =tree-search=: returns a pointer to a node whose key =k= of one exists, 
  otherwise returns =NIL=:

#+BEGIN_SRC 
recursive-tree-search(x, k)
    if x == NIL or k == x.key
        return x
    if k < x.key
        return tree-search(x.left, k)
    else
        return tree-search(x.right, k)

iterative-tree-search(x, k)
    while x != NIL and k != x.key
        if k < x.key
            x = x.left
        else
            x = x.right
    return x
#+END_SRC

- =minimum= / =maximum=: returns a pointer to the minimum element in a tree

#+BEGIN_SRC 
iterative-tree-minumum(x):
    while x.left != NIL
        x = x.left
    return x

recursive-tree-minimum(x):
    if x.left == NIL
        return x
    else
        return recursive-tree-minimum(x.left)

iterative-tree-maximum(x):
    while x.right != NIL
        x = x.right
    return x
#+END_SRC

- =sucessor= / =predecessor=: $O(h)$

#+BEGIN_SRC 
tree-successor(x)
    if x.right != NIL
        return tree-minumum(x.right)
    y = x.parent
    # if x.parent is on x's left, then x.parent and 
    # the left subtree of x.parent can not be greater than x
    # otherwise  continue to search until there is a parent node
    # is on the right (when x is in the left subtree of this ancestor node)
    while y != NIL and x == y.right
        x = y
        y = y.parent
    return y
#+END_SRC

We can implement the dynamic-set operations =search=, =minimum=, =maximum=, 
=sucessor= and =predecessor= so that each one runs in $O(h)$ time on a binary
tree of height $h$.

- =tree-insert=: takes a node =z= and inserts it into an appropriate position 
  in the tree

#+BEGIN_SRC 
tree-insert-iterative(T, z)
    y = NIL
    x = T.root  
    while x != NIL      # This NIL occupies where z should be placed
        y = x           # save previous root node, a trailing pointer
        if z.key < x.key
            x = x.left  # change root node so that z is under this subtree
        else
            x = x.right
    z.parent = y
    if y == NIL
        T.root = z
    else if z.key < y.key
        y.left = z
    else
        y.right = z
#+END_SRC

- =tree-delete=: three basic cases
  1. has no children. simply remove it
  2. has just one child. Elevate that child to take over the position.
  3. has two children. Then find its successor in its right subtree and have it
     take its place.

#+BEGIN_SRC
# replaces one subtree as a child of its parent with another subtree
# replaces the subtree rooted at node u with the subtree rooted at node v
transplant(T, u, v)
  if u.parent == NIL
      T.root = v
  else if u == u.parent.left
      u.parent.left = v
  else
      u.parent.right = v
  if v != NIL
      v.parent = u.parent

tree-delete(T, z)
  # Four cases
  if z.left == NIL
      transplant(T, z, z.right) # including the case where z has no children
  else if z.right == NIL
      transplant(T, z, z.left)
  else        # has both children
      y = tree-minimum(z.right)
      if y.parent != z  
          transplant(T, y, y.right) # replace y with its right child
          y.right = z.right # put y in place of z
          z.right.parent = y
      # replace z with y
      transplant(T, z, y)
      y.left = z.left
      y.left.parent = y
#+END_SRC

- Theorem: The dynamic-set operation =insert= and =delete= can be implemented
  in a BST of height $h$ so that each one runs in $O(h)$.

**** Implementation

#+include: "./python-implementation/BST_sort.py" src python

*** Balanced BST

Balanced BST maintains $h=O(\lg n)$, which makes all operations run in $O(\lg n)$
time.

**** Red-Black Trees

A red-black tree is a binary search tree with one extra attribute per node: 
/color/, which can be either /red/ or /black/. Red-black trees ensure that
no such plan is more than twice as long as any other, so that the tree is 
approximately /balanced/.

/Red-black properties/: =NIL= pointers are regarded as being pointers to leaves,
which can be replaced by a single sentinel node.
  1. Every node is either red or black
  2. The root is black
  3. Every leaf (=NIL=) is black
  4. If a node is red, then both its children are black (at least half of the nodes
     on any simple path form the root to a leaf are black)
  5. For each node, all simple paths from the node to descendant leaves contain
     the same number of black nodes, not including the starting node, called
       /black-height/, denoted by $bh(x)$.

- Lemma: A red-black tree with $n$ internal nodes (real nodes excluding the =NIL= nodes) 
  has height (not black height) at most $2\lg (n+1)$

#+TODO: proof using induction on the assumption that the subtree rooted at any node $x$ contains at least $2^{bh(x)} - 1$ internal nodes. and then consider property 4.

From this lemma, we can implement =search=, =minimum=, =maximum=, =successor= 
and =predecessor= in $O(\lg n)$ time on red-black trees.

***** Rotations

Rotations are local operations in a search tree that preserves the binary-search-tree
property. Rotations run in $O(1)$ time.

#+BEGIN_SRC 
                            Left-rotate
             +---+      <-----------------+     +---+
        +----+ y +---+                     +----+ x +---+
        |    +---+   |                     |    +---+   |
        |            |                     +            |
      +-+-+          +                   alpha          |
  +---+ x +---+    gamma                              +-+-+
  |   +---+   |                                  +----+ y +-----+
  +           +            Right-rotate          |    +---+     |
alpha       beta        +----------------->      |              |
                                                 +              +
                                               beta           gamma
#+END_SRC

#+BEGIN_SRC 
left-rotate(T, x)
    y = x.right
    x.right = y.left
    # modify beta
    if y.left != T.nil
          y.left.parent = x
    # modify y
    y.parent = x.parent
    # modify x.parent
    if x.parent == T.nil
        T.root = y
    else if x == x.parent.left
        x.parent.left = y
    else
        x.parent.right = y
    # modify x and y
    y.left = x
    x.parent = y
#+END_SRC

- =rb-insert=: $O(\lg n)$.

#+BEGIN_SRC 
# All NILs are replaced by T.nil, z.color is set to RED
rb-insert(T, z)
    y = T.nil
    x = T.root
    while x != T.nil
        y = x
        if z.key < x.key
            x = x.left
        else
            x = x.right
    z.parent = y
    if y == T.nil
        T.root = z
    else if z.key < y.key
        y.left = z
    else
        y.right = z
    z.left = T.nil
    z.right = T.nil
    z.color = RED
    rb-insert-fixup(T, z)

rb-insert-fixup(T, z)
    while z.parent.color == RED
        if z.parent == z.parent.parent.left
            y = z.parent.parent.right
            # case 1 corrects the lone violation of property 4
            if y.color == RED               # case 1 z's uncle is red
                z.parent.color = BLACK      # case 1 simply color the upper
                y.color = BLACK             # case 1 two nodes black and 
                z.parent.parent.color = RED # case 1 grandparent red
                z = z.parent.parent         # case 1
            else  
                if z == z.parent.right      # case 2
                    z = z.parent            # case 2
                    left-rotate(T, z)       # case 2
                z.parent.color = BLACK      # case 3
                z.parent.parent.color = RED # case 3
                right-rotate(T, z.parent.parent)  # case 3
        else
            (z is on the right side of z's grandparent,
              same as left, but with left and right reversed)
    T.root.color = BLACK
#+END_SRC

Property 2 may be violated if =z= is the root and also property 4 if =z= 's parent
is red. The =while= loop in the fixup procedure maintains three-part invariant
at the start of each iteration of the loop:
    1. Node =z= is red
    2. If =z.parent= is the root, then =z.parent= is black
    3. If the tree violates any of the red-black properties, then it violates
       at most one of them, and the violation is either of property 2 (=z= is 
       the root) or property 4 (both =z= and =z.parent= are red)

There are three cases (actually six, depending on whether =z.parent= is a left 
child or a right child) where we enter the =while= loop.
  1. =z='s uncle =y= is red
  2. =z='s uncle is black and =z= is a right child
  3. =z='s uncle is black and =z= is a left child

#+TODO: loop invariant proof

#+TODO: rb-deletion

***** Implementation

#+include: "./python-implementation/red_black_tree.py" src python

**** AVL Tree

The AVL trees are more balanced compared to RB-Tree, but they may cause more
rotations during insertion and deletion. The Red Black trees should be 
preferred if many frequent insertions and deletions are involved.

For every node, heights of left and right children differ by at most $\pm 1$.
Nil tree is treated as height $-1$. Each node stores its height (or difference
in heights).

Worst when every node differs by 1, let $N_{h}$ nodes in height-=h= AVL tree.

\begin{aligned}
N_{h} &	=N_{h-1}+N_{h-2}+1 \\
&	>2N_{h-2} \\
N_{h} & > 2^{h/2} \\
h & < 2 \lg N_{h}
\end{aligned}

- =avl_insert=: insert as in simple BST, work the way up tree, restore AVL property.
  $\Theta(n \lg n)$

#+TODO: insertion, deletion

***** Implementation

#+TODO: python implementation

#+include: "./python-implementation/avl_tree.py" src python


** Hash Tables

Under reasonable assumptions, the average time to search for an element in a 
hash table is $O(1)$.

When the number of keys actually stored is small relative to the total number of
possible keys, hash tables become an effective alternative to directly addressing
an array. Instead of using the key as an array index directly, the array index
is computed from the key.

*** Direct-Address Tables

Direct addressing works well when the universe $U$ of keys is reasonably small.
Direct-address table is basically an array, of which each position (/slot/) 
corresponds to a key in the universe $U$.

- =direct-address-search(T, k)=: =return T[k]=

- =direct-address-insert(T, x)=: =T[x.key] = x=

- =direct-address-delete(T, x)=: =T[.key] = NIL=

Each of these operations takes $O(1)$ time.

*** Hash Tables

A hash table requires much less storage than a direct address table when the
set $K$ of keys stored in a dictionary is much smaller than the universe $U$
of all possible keys.

/hash function/: $h\colon U\to \{0, 1, ..., m-1 \}$, $h$ maps the universe $U$
of keys into the slots of a hash table $T[0..m-1]$, where the size $m$ of the
hash table is typically much less than $|U|$. $h(k)$ is the /hash value/ of key
$k$.

/collision/: two keys hashing to the same slot. A suitable hash function may 
eliminate collisions. Avoiding collisions altogether is impossible since hashing
typically has a larger domain than a larger range.

/Chaining/ and /open addressing/ are two methods to solve collision. Chaining
is more commonly selected as a collision resolution technique when keys must be
deleted.

/Chaining/: all the elements that hash to the same slot into the same linked
 list. Slot $j$ contains a pointer to the head of the list of all stored 
elements. If there are no such elements, slot $j$ contains =NIL=.

- =chained-hash-insert(T, x)=: insert =x= at the *head* (so that we don't need to
  traverse the list) of list =T[h(x.key)]=

- =chained-hash-search(T, x)=: search for an element with key $k$ in list =T[h(k)]=

- =chained-hash-delete(T, x)=: delete =x= from the list =T[h(x.key)]=

/load factor/: given a hash table $T$ with $m$ slots that stores $n$ elements,
the load factor $\alpha$ for $T$ is $n/m$, the average number of elements 
stored in a chain.

/simple uniform hashing/: any given element is equally likely to hash into any
of the $m$ slots, independently of where any other element has hashed to.

- theorem: In a hash table in which collisions are resolved by chaining, an 
  unsuccessful search takes average-case time $\Theta(1+\alpha)$, a successful 
  search takes average-case time $\Theta(1+\alpha)$, under the assumption of
  simple uniform hashing.

#+TODO: proof

*** Hash functions

A good hash function satisfies approximately the assumption of simple uniform
hashing: each key equally likely to hash to any of the $m$ slots, independently
of where any other key has hashed to. Some application s of hash functions
might require stronger properties than are provided by simple uniform hashing.

Most hash functions assume that the universe of keys is the set of natural 
numbers. If the keys are not natural numbers, find a way to interpret them as
natural numbers.

**** The Division Method

$$
h(k) = k \mod m
$$

$m$ should not be a power of 2, otherwise the hash value depends on certain lower
bits of $k$ rather than all bits of $k$. A prime not too close to an exact
 power of 2 is often a good choice.

**** The Multiplication Method

$$
h(k) = \lfloor m(k A \mod 1)\rfloor
$$

where $kA \mod 1$ means $kA - \lfloor kA\rfloor$, $0 < A < 1$.

An advantage of the multiplication method is that the value of $m$ is not 
critical. It is typically a power of 2. $\sqrt{5}-1/2$ may be a good choice 
for $A$.

*** Open Addressing

Each table entry contains either an element of the dynamic set or =NIL=. When
searching for an element, the table slots are systematically examined until
either the desired element is found or it is ascertained that the element is 
not in the table. In open addressing,, the hash table can be fill up. The load
factor can never exceed $1$.

The advantage of open addressing is that it avoids pointers altogether. Instead
of following pointers, the sequence of slots to examine is the computed.

To perform insertion, /probe/ the hash table until an empty slot in which to
put the key is found. The sequence of position  probed depends on the key being
inserted.

The hash function becomes

$$
h\colon U\times\left\{ 0,1,\dots,m-1\right\} \to\left\{ 0,1,\dots,m-1\right\} 
$$

The /probe sequence/ $h\left(k,0\right),h\left(k,1\right),\dots,h\left(k,m-1\right)$
must be a permutation of $0, 1, \dots ,m-1$ so that every hash table position
is eventually considered as a slot for a new key as the table fills up.

#+BEGIN_SRC 
hash-insert(T, k)
    i = 0
    repeat
        j = h(k, i)
        if T[j] == NIL
            T[j] = k
            return j
        else
            i = i + 1
    until i == m
    error "Hash table overflow"

hash-search(T, k)
    i = 0
    repeat
        j = h(k, i)
        if T[j] == k
            return j
        i = i + 1
    until T[j] == NIL or i == m
    return NIL
#+END_SRC

Deletion from an open-address hash table is difficult. One way is to set the 
deleted slot to =deleted= instead of =NIL=.

/uniform hashing/: the probe sequence of each key is equally likely to be any
of the $m!$ permutations of $0, 1, \dots, m-1$.

Three commonly used techniques to compute the probe sequences:

1. /linear probing/: given an ordinary hash function 
  $h^{\prime}\colon U\to\left\{ 0,1,\dots,m-1\right\}$ (/auxiliary hash function/),
  linear probing uses $h\left(k,i\right)=\left(h^{\prime}\left(k\right)+i\right)\mod m$ 
  for $i = 0, 1, \dots, m-1$. Linear probing suffers from /primary clustering/.
  Long runs of occupied slots build up, increasing the average search time.

2. /quadratic probing/: $h\left(k,i\right)=\left(h^{'}\left(k\right)+c_{1}i+c_{2}i^{2}\right)\mod m$
  where $h^{'}$ is an auxiliary hash function, $c_1$ and $c_2$ are positive 
  auxiliary constants and $i=0,1,\dots,m-1$. The values of $c_1$, $c_2$ and $m$
  are constrained to make full use of the hash table. $h\left(k_{1},0\right)=h\left(k_{2},0\right)$
  implies $h\left(k_{1},i\right)=h\left(k_{2},i\right)$, leading to a milder 
  form of clustering, called /secondary clustering/.

3. /double hashing/: One of the best methods available for open addressing.
   $h\left(k,i\right)=\left(h_{1}\left(k\right)+ih_{2}\left(k\right)\right)\mod m$
   where both $h_1$ and $h_2$ are auxiliary hash functions. The value $h_2(k)$
   must be relatively prime to the hash-table size $m$ for the entire hash table.
   (e.g. Let $m$ be a power of $2$ and design $h_{2}$ so that it always
   produces an odd number). 

#+TODO: analysis of open-address hashing


** Augmenting Data Structures

* Sorting and Trees

Input: array =A[1...n]= of numbers

Output: permutation =B[1...n]= of =A= such that 
$B[1] \leq B[2] \leq \cdots \leq B[n]$

A sorting algorithm sorts /in place/ if only a constant number of  elements 
of the input array are ever stored outside the array.

Insertion sort, merge sort, heap sort and quick sort are all comparison sorts:
they determine the sorted order of the input array by comparing elements. The 
decision tree model shows the performance limitation of comparison sorts.

The lower bound of $\Omega(n\lg n)$ can be beaten if more information about the
sorted order of the input can be gathered by means other than comparing elements.
e.g. counting sort, radix sort, bucket sort.

| Algorithm       | Worst-case running time     | Average-case/expected running time     | 
|-----------------+-----------------------------+----------------------------------------+
| Insertiion sort | $\Theta(n^{2})$             | $\Theta(n^{2})$                        | 
| Merge Sort      | $\Theta\left(n\lg n\right)$ | $\Theta\left(n\lg n\right)$            | 
| Heapsort        | $O\left(n\lg n\right)$      | -                                      | 
| Quicksort       | $\Theta\left(n^{2}\right)$  | $\Theta\left(n\lg n\right)$ (expected) | 
| Counting sort   | $\Theta(k+n)$               | $\Theta(k+n)$                          | 
| Radix Sort      | $\Theta(d(n+k))$            | $\Theta(d(n+k))$                       | 
| Bucket sort     | $\Theta(n^{2})$             | $\Theta(n)$ (average case)             | 


** Sorting

*** Insertion Sort

A fast in-place sorting algorithm for small input sizes

- Pseudocode

#+BEGIN_SRC python
for j = 2 to A.length
    key = A[j] // insert A[j] into the sorted sequence A[1...j-1].
    i = j - 1
    while i > 0 and A[i] > key
        A[i+1] = A[i]
        i = i - 1
    A[i+i] = key  
#+END_SRC

Worst-case running time $\Theta(n^{2})$ for a reversed-ordered array.

An improvement to the naive version of insertion sort is to use binary
 search on the already sorted part of the array, which yields 
$\Theta(n\lg n)$ running time if the swap operation is cheap.
**** Implementations

#+INCLUDE: "python-implementation/insertion_sort.py" src python

#+begin_src cpp
// The boost implementation

template < class Iter_t, typename Compare = compare_iter < Iter_t > >
static void insert_sort (Iter_t first, Iter_t last,
                         Compare comp = Compare())
{
    //--------------------------------------------------------------------
    //                   DEFINITIONS
o    //--------------------------------------------------------------------
    typedef value_iter< Iter_t > value_t;

    if ((last - first) < 2) return;


    for (Iter_t it_examine = first + 1; it_examine != last; ++it_examine)
    {
        value_t Aux = std::move (*it_examine);
        Iter_t it_insertion = it_examine;

        while (it_insertion != first and comp (Aux, *(it_insertion - 1)))
        {
            *it_insertion = std::move (*(it_insertion - 1));
            --it_insertion;
        };
        *it_insertion = std::move (Aux);
    };
};
#+end_src

#+INCLUDE: "./c++-implmentation/sort/insertion_sort.cc" src C++

*** Merge Sort

The =merge= procedure it uses does not operate in place.

Pseudocode

#+BEGIN_SRC python
def merge-sort(A, p, r):
    if p < r:
        q = (p + r) // 2

        merge-sort(A, p, q)
        merge-sort(A, q+1, r)
        merge(A, p, q, r)

def merge(A, p, q, r):
    n1 = q - p + 1
    n2 = r - q
    # copy for inplace merge
    let L[1..n1 + 1] and R[1..n2 + 1] be new arrays
    for i = 1 to n1:
        L[i] = A[p + i - 1]
    for j = 1 to n2:
        R[j] = A[q + j]
    L[n1 + 1] = inf # sentinel 
    R[n2 + 1] = inf
    i = 1
    j = 1

    # merge starts here
    for k = p to r:
        if L[i] <= R[j]:
            A[k] = L[i]
            i = i + 1
        else:
            A[k] = R[j]
            j = j + 1
#+END_SRC

The =merge= procedure takes time $\Theta(n)$.

The worst case running time is 

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n=1\\
2T\left(n/2\right)+\Theta\left(n\right) & \text{if }n>1
\end{cases}
$$

resolved to $T\left(n\right)=\Theta\left(n\lg n\right)$.

**** Implementation

Python implementation

#+include: ./python-implementation/merge_sort.py src python

Home brewed implementation

#+include: ./c++-implmentation/sort/merge_sort.cc src C++

libstdc++ =std::merge= implementation

#+BEGIN_SRC C++
  template<typename _InputIterator1, typename _InputIterator2,
	   typename _OutputIterator, typename _Compare>
    _OutputIterator
    __merge(_InputIterator1 __first1, _InputIterator1 __last1,
	    _InputIterator2 __first2, _InputIterator2 __last2,
	    _OutputIterator __result, _Compare __comp)
    {
      while (__first1 != __last1 && __first2 != __last2)
	{
	  if (__comp(__first2, __first1))
	    {
	      *__result = *__first2;
	      ++__first2;
	    }
	  else
	    {
	      *__result = *__first1;
	      ++__first1;
	    }
	  ++__result;
	}
      return std::copy(__first2, __last2,
		       std::copy(__first1, __last1, __result));
    }  
#+END_SRC

*** Heap Sort

*priority queue*: a data structure implementing a set S of elements, each associated with a key, supporting the following operations
  + =insert(S, x)=
  + =max(S)=
  + =extract_max(S)=: return the element with the largest key and remove it
  + =update_key(S, x, k)=: update the value of element x's key to new value k

*heap*: an array visualized as a nearly complete binary tree, that implements a
priority queue. The height of a binary heap (the longest simple downward path
from the root to a leaf) is $\lfloor \lg n \rfloor$ and at most $\lceil n/2^{h+1}\rceil$
nodes of any height $h$.
  
=heapify=: correct A[i], which violates the heap property, 
$O(\log n)$ from $T\left(n\right)\leq T\left(2n/3\right)+\Theta\left(1\right)$ (master theorem).

#+BEGIN_SRC python
def max_heapify(A, i):
    l = left(i)
    r = right(i)
    if l <= heap_size(A) and A[l] > A[i]:
          largest = l
    else:
          largest = i
    if r <= heap_size(A) and A[r] > A[largest]:
          largest = r
    if largest != r:
          swap(&A[i], &A[largest])
          max_heapify(A, largest)
#+END_SRC

=build_heap=: converts =A[1...n]= to a heap, $O(n)$. In general, =max_heapify=
takes $O(l)$ for the nodes that are $l$ levels above the leaves. The loop variant
is: at the start of each iteration, each node $i+1$, $i+2$, ..., n is the root of
a heap (which is exactly the preconfition for =heapify=). 

#+BEGIN_SRC python
def build_max_heap(A):
    for i = n // 2 downto 1:    # n//2 is the last non-leaf node
        do max_heapify(A, i)
#+END_SRC


- *min/max heap property*: the key of a node is greater/less than the keys of 
its children. The /root/ of the tree is the /first/ element (i=1) in the array,

#+begin_example
# one-based index  zero-based index
parent(i) = i/2   # (i-1)/2 
left(i) = 2i      # 2i+1
right(i) = 2i+1   # 2i+2
#+end_example

- *heap sort*: every iteration involves a =swap= and a =heapify= operation, hence takes $O(n\log n)$ overall.
  1. build a heap from unordered array
  2.  find maximum element A[1]
  3. swap elements A[n] and A[1], the max element is now at the end of the array
  4. discard node =n= form heap (by decrementing heap size)
  5. new root may violate heap property, but its children are heaps, run =heapify= to fix it.
  6. go to step 2 unless heap is empty

#+BEGIN_SRC 
heap_sort(A):
    build_heap(A)
    for i = A.len downto 2:
        exchange A[i] with A[i]
        A.heap_size = A.heap_size - 1
        heapify(A, 1)
#+END_SRC


**** Implementation

#+include: "./python-implementation/heap_sort.py" src python

*** BST Sort

BST sort can do fast insertion into sorted list.

*** Quicksort

inplace; $\Theta(n^{2})$ worst case, $\Theta(n\lg n)$ expected running time; the
constant factors are small; divide-and-conquer

1. *divide*: partition the array into two (possibly empty) subarrays =A[p..q-1]= and A[q+1..r] such that each element of =A[p..q-1]= is less than or equal to A[q], which is, in turn, less than or equal to each element of =A[q+1..r]=. Compute the index =q= as part of this partitioning procedure

2. *conquer*: sort the two subarrrays =A[p..q-1]= and =A[q+1..r]= by recursive calls

3. *combine*: all subarrays are already sorted, no work for combining them.

#+BEGIN_SRC 
quicksort(A, p, r)
  if p < r
    q = partition(A, p, r)
    quicksort(A, p, q-1)
    quicksort(A, q+1, r)
#+END_SRC

The key to the algorithm is the =partition= procedure, which rearranges the subarray =A[p..r]= in place:

#+BEGIN_SRC 
partition(A, p, r)
  x = A[r]
  i = p - 1     # the last element of the first half subarray
  for j = p to r-1  # the boundary of the rearranged part of the array
    if A[j] <= x
      i = i + 1
      exchange A[i] with A[j] # place the smaller one to the left
  exchange A[i+1] with A[r] # so that the pivot divides the two subarrays
  return i+1  # index of the pivot
#+END_SRC

There are four regions maintained by this procedure:

#+BEGIN_SRC 
| <= pivot | > pivot | unrestricted | pivot |  
#+END_SRC


The loop invariant is: At the beginning of each iteration of the loop, for any
array index $k$:
  1. If $p\leq k\leq i$, then $A[k] \leq x$.
  2. If $i+1 \leq k\leq j-1$ then $A[k] > x$
  3. If $k = r$, then $A[k] = x$

The running time of =partition= on =A[p..r]= is $\Theta(n)$ where $n=r-p+1$.

*** Some other sorting algorithms

**** Selection Sort

pinplace comparison sort; $O(n^{2})$ comparison, $O(n)$ swaps (worst-case and 
average-case); generally performs worse than insertion sort; 

The array is divided into a sorted subarray and an unsorted subarray. The algorithm
proceeds by finding the smallest element in the unsorted subarray, exchanging it 
with the leftmost unsorted element and moving the subarray boundaries one element 
to the right.

#+include: "./python-implementation/selection_sort.py" src python

**** Bubble/Sinking Sort

inplace; $O(n^{2})$ comparisons and swaps (worst-case and average-case); primarily
used as an education tool

The algorithm steps through the list, compares adjacent elements and swaps them
if they are in the wrong order.

#+include: "./python-implementation/bubble_sort.py" src python

** Reading Notes

1.2, 2.1-2.3, 4.3-4.6;

*** Loop Invariants and The correctness of an algorithm

To show the algorithm is correct, we require that for such a loop, the
 properties of ~A[i...j-1]~ (a loop invariant, here sortedness) must satisfy:

- Initialization: it is true prior to the first iteration of the loop;

- Maintenance: If it is true before an iteration of the loop, it remains
 true before the next iteration;

- Termination: When the loop terminates, the invariant gives us a useful
 property that helps show that the algorithm is correct. (a point where 
mathematical induction should stop)

*** Algorithm Analysis

*analyzing an algorithm*: predicting the resources that the algorithm requires 
such as memory, communication bandwidth, or computer hardware, but most often
computational time.

*running time of an algorithm*: the number of primitive operations or "steps" 
executed. More often, we consider the *rate/order of growth* of the 
running time. We usually consider one algorithm to be more efficient than
 another if its worst-case running time has a lower order of growth.

Even for inputs of a given size, an algorithm's running time may depend on
 which input of that size is given. The best case for insertion sort occurs
if the array is already sorted, resulted in a linear running time. The 
worst case running time is a quadratic function. Normally, we concentrate
on finding only the *worst-case running time* (The average case is often
 rougly as bad as the worst case, as in the case of insertion sort). When
analyzing the average-case running time, the technique of probabilistic 
analysis is applied.

*** Algorithm Designing

- /incremental/ approach: e.g. insertion sort

- /divide-and-conquer/ approach: to solve a given problem, an algorithm
 calls itself recursively one or more times to deal with closely related
subproblems. e.g. merge sort
    + Divide the problem into a number of subproblems that are smaller 
    instances of the same problem
    + conquer the subproblem by solving them recursively (recursive cases).
    If the subproblem sizes are small enough, solve them in a straightforward 
    manner.
    + combine the solutions to the subproblems into the solution for the 
    original problem.

The running time of a recursive call is described by a /recurrence equation/
or /recurrence/, which describes the overall running time on a problem
of size $n$ in terms of the running time on smaller inputs.

$$
T\left(n\right)=\begin{cases}
\Theta\left(1\right) & \text{if }n\leq c\\
aT\left(n/b\right)+D\left(n\right)+C\left(n\right) & \text{otherwise}
\end{cases}
$$

where $c$ is the size small enough for the straightforward solution; 
$a$ is the number of subproblems; $D(n)$  is time to divide the problem
 and $C(n)$ is the time to combine the solutions to the subproblems.

This equation can be solved intuitively sometimes by drawing a 
/recurrion tree/.

*** More about Divide-and-Conquer (Chap.4)

**** Three Methods for solving recurrences

****** Substitution Method
        
1. Guess the form of the solution

2. Use mathematical induction (the strong version is more often used) to find
 the constants and show that the solution works.

The guess is that $T(n) = O(n \lg n)$ for the following recurrence

$$
T\left(n\right)=2T\left(\lfloor n/2\rfloor\right)+n
$$

- Assuming that this bound holds for all positive $m < n$ for an appropriate 
choice of $c$, in particular for $m = \lfloor n / 2\rfloor, yielding 
$T\left(\lfloor n/2\rfloor\right)\leq c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)$
, substituting into the recurrence yields

\begin{aligned}
T\left(n\right)	& \leq2(c\lfloor n/2\rfloor\lg\left(\lfloor n/2\rfloor\right)+n \\
	& \leq cn\lg\left(n/2\right)+n \\
	& =cn\lg n-cn\lg2+n \\
	 & =cn\lg n-cn+n \\
	 & \leq cn\lg n \\
\end{aligned}

- Since we only need to prove $T(n) \leq cn\lg n$ for $n \geq n_{0}$, the 
troublesome boundary condition $T(1) = 1$ is removed form consideration in the
induction.

Guessing a solution takes experience and creativity. Sometimes, a little 
algebraic manipulation can make an unknown recurrence similar to what has been
seen.

$$
T\left(n\right)=2T\left(\lfloor\sqrt{n}\rfloor\right)+\lg n
$$

Renaming $m = \lg n$ yields 

$$
T\left(2^{m}\right)=2T\left(2^{m/2}\right)+m
$$

Renaming $S(m) = T(2^{m})$

$$
S\left(m\right)=2S\left(m/2\right)+m
$$

Then 

$$
T\left(n\right)=T\left(2^{m}\right)=S\left(m\right)=O\left(m\lg m\right)=O\left(\lg n\lg\lg n\right)
$$

****** Recursion-tree Method

A recursion tree is best used to generate a good guess, which can then be 
verified by the substitution method.

It converts the recurrence into a tree whose nodes represent the costs incurred
at various levels of the recursion and uses techniques for bounding summations
to solve the recurrence. Each node represents the cost of a single subproblem
somewhere in the set of recursive function invocations. We sum the costs within
each level of the tree to obtain a set of per-level costs.

Floors and ceilings usually do not matter when solving recurrences, so finding
an upper bound is easier without them.


****** Master Method

Provides bounds for recurrences of the form

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $a \geq 1$, $b > 1$ and $f(n)$ is a given function.

- (Master theorem) Let $a \geq 1$ and $b \ge 1$ be constants, let $f(n)$ be a
function, and let $T(n)$ be defined on the nonnegative integers by the 
recurrence

$$
T\left(n\right)=aT\left(n/b\right)+f\left(n\right)
$$

where $n/b$ is interpreted to be mean either $\lfloor n/b \rfloor$ or 
$\lceil n/b \rceil$. Then $T(n)$ has the following asymptotic bounds

1. If $f\left(n\right)=O\left(n^{\log_{b}a-\epsilon}\right)$ for some constant
 $\epsilon > 0$ then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$.

2. If $f\left(n\right)=\Theta\left(n^{\log_{b}a}\right)$, then $T\left(n\right)=\Theta\left(n^{\log_{b}a}\lg n\right)$

3. If $f\left(n\right)=\Omega\left(n^{\log_{b}a+\epsilon}\right)$ for some  constant $\epsilon > 0$ and if $a f(n/b) \leq cf(n)$ for some constant $c<1$
and all sufficiently large $n$, then $T\left(n\right)=\Theta\left(f\left(n\right)\right)$.

#+TODO: Proof of the master theorem

6.1-6.4

***** The structure of the data

Typically, the data that are sorted are usually part of a collection of data 
called a /record/. Each record contains a /key/, which is the value to be sorted.
The remainder of the record consists of /satellite data/, which are usually 
carried around with the key.

***** Order Statistics

The $i$-th order statistic of a set of $n$ numbers is the $i$-th smallest number
in the set. We can find this by sorting in $\Omega(n\lg n)$. More efficient 
algorithms are available.



#+TODO: 

10.4, 12.1-12.3

#+TODO: 

13.2, 14

#+TODO: 

8.1-8.3

#+TODO: 

****** 


* Numerics

#+TODO: 

* Graphs

** Representation

The adjacency-list representation provides a compact way to represent /sparse/
graphs. The adjacency-matrix representation is preferred when its dense.

/Adjacency-list representation/: an array $Adj$ of $|V|$ lists, one for each 
vertex in $V$. For each $u\in V$, the adjacency list $Adj[u]$ contains all the
vertices $v$ s.t. there is an edge $(u, v) in E$. The amount of memory required
is $\Theta(V + E)$. The weight of an edge can also be stored in the list. The
adjacency-list representation is robust in that it can be modified to support
many other graph variants. A potential disavantage is that it provides no 
quicker way to determine whether a given edge $(u, v)$ is present in the graph
than to search for $v$ in the adjacency list $Adj[u]$.

$|E|$: for a directed graph, the sum of the lengths of all the adjacency lists,
for an undirected graph, half the sum.

/Adjacency-matrix representation/: A matrix of a $|V| \times |V|$ matrix 
$A = (a_{ij})$ s.t.

$$
a_{ij}=\begin{cases}
1 & \text{if }\left(i,j\right)\in E\\
0 & \text{otherwise}
\end{cases}
$$

For an undirected graph, $A = A^{T}$.

Adjacency matrix representation is simpler, preferred when graphs are reasonably
small.

** Elementary Graph Algorithms

*** Breadth-first search

Given a graph $G=(V, E)$ and a distinguished source vertex $s$, breadth-first
search systematically explores the edges of $G$ to discover every vertex that 
is reachable from $s$. It expands the frontier between discovered and undiscovered
 vertices uniformly across the breadth of the frontier. The algorithm all 
vertices at distance $k$ from $s$ before discovering any vertices at distance
$k+1$.

To keep track of progress, each vertex is colored with /white/ (undiscovered),
 /gray/ or /black/. All vertices to black vertices have been discovered; gray
vertices may have some adjacent white vertices: they represent the frontier
between discovered and undiscovered vertices.

Breadth-first search constructs a breadt
h-first tree, initially containing only
its root (the source vertex $s$). Whenever the search discovers a white vertex
$v$ when scanning an already discovered vertex $u$ (/predecessor/ or /parent/).
A vertex is discovered at most once, thus it has at most one parent.

Assuming we represent the input graph using adjacency lists:

#+BEGIN_SRC 
# u.color: color of each vertex
# u.pre: predecessor
# u.dist: distance from the source s to vertex u
# Q: a FIFO
BFS(G, s)
    for each vertex u in G.V - {s}
        u.color = WHITE
        u.dist = inf
        u.pre = NIL
    
    s.color = GRAY
    s.d = 0
    Let Q be an empty queue
    enqueue(Q, s)
    while Q is not empty # invariant: the queue consists of the set of gray vertices
        u = dequeue(Q)
        for each v in G.Adj[u]
            if v.color == WHITE
                v.color = GRAY
                v.dist = u.d + 1
                v.pre = u
                enqueue(Q, v)
        u.color = BLACK
#+END_SRC

The total time devoted to queue operations is $O(V)$. Each adjacency list is 
scanned at most once. Since the sum of the lengths of all the adjacency lists
is $\Theta(E)$, the total running time of BFS is $O(V+E)$. Breadth-first search
runs in time linear in the size of the adjacency-list representation of $G$.


* Shortest Paths

#+TODO: 

* Dynamic Programming

A dynamic-programming algorithm solves each subsubproblem just once and then
saves its answer in a table, thereby avoiding the work of recomputing the answer
every time it solves each subsubproblem.

We typically apply dynamic programming to /optimization problems/.

When developoing a dynamic-programming algorithm, a sequence of four steps are 
followed in:
  1. Characterize the structure of an optimal solution
  2. Recursively define the value of an optimal solution
  3. Compute the value of an optimal solution, typically in a bottom-up fashion
  4. Construct an optimal solution from computed information

** Rod Cutting

/Problem formulation/: given a rod of length $n$ and a table of prices $p_i$ for $i=1, 2, 3, \dots , n$
 determine the maximum revenue $r_{n}$ obtainable by cutting up the rod and 
selling the pieces. If the price for a rod of length $n$ is large enough, an
optimal solution may require no cutting at all.

We have an independent option of cutting or not cutting at distance $i$ from the
left end for $i=1,2,\dots ,n-1$, giving $2^{n-1}$ different ways. An optimal 
decomposition 

$$
n = i_{1} + i_{2} + \cdots + i_{k}
$$

of the rod into pieces of lengths $i_{1}, i_{2}, \dots, i_{k}$ provides maximum
corresponding revenue

$$
r_{n} = p_{i_1} + p_{i_2} + \cdots + p_{i_k}
$$

We can frame the values $r_n$ for $n\geq 1$ in terms of optimal revenues from
shorter rods:

$$
r_{n} = \max (p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \dots , r_{n-1} + r_1)
$$

To solve the original problem of size $n$, we solve smaller problems of the same type
 but of smaller sizes.

/optimal substructure/: optimal solutions to a problem incorporate optimal 
solutions to related subproblems, which may be solved independently.

We can also approach the problem by seeing the first part as indecomposable. 
Only the second part can be further divided (or of zero length).

$$
r_{n}=\underset{1\leq i\leq n}{\max}\left(p_{i}+r_{n-i}\right)
$$

which leads to the following recursive implementation:

#+BEGIN_SRC 
cut-rod(p, n)
    if n == 0
        return 0
    q = -INF
    for i = 1 to n
        q = max(q, p[i] + cut-rod(p, n-i))
    return q
#+END_SRC

The problem of this algorithm is that it calls itself recursively over and over
again with the same parameter values. It solves the same subproblems repeatedly.
Since when $n=10$ and when $n=9$, there is a =cut-rod(p, 8)= call.

$$
T(n) = 1 + \sum_{j=0}^{n-1} T(j) \\
T(0) = 1
$$


which solves to $T(n) = 2^{n}$.

There are two equivalent ways to implement a dynamic-programming approach:

1. /top-down with memoization/: write the procedure recursively in a natural 
  manner, but modified to save the result of each subproblem, usually in an 
  array or hash table. The procedure first checks to see whether it has 
  previously solved this subproblem.

2. /bottom-up method/: depends on some natural notion of the size of a 
  subproblem s.t. solving any particular subproblem depends only on solving
  smaller subproblems. The subproblems are sorted by size and solved in size
  order, smallest first. When a problem is encountered, all its prerequisite
  subproblems have been solved.

#+BEGIN_SRC 
memoize-cut-rod(p, n)   # 
  let r[0..n] be a new array
  for i = 0 to n
      r[i] = -inf
  return memoized-cut-rod-aux(p, n, r)

memoized-cut-rod-aux(p, n, r) 
  if r[n] >= 0
      return r[n]
  if n == 0
      q = 0
  else
      q = -inf
      for i = 1 to n
          q = max(q, p[i] + memoized-cut-rod-aux(p, n-i, r))  # returns immediately
  r[n] = q
  return q

botton-up-cut-rod(p, n) # \Theta(n^{2})
    let r[0..n] be a new array
    r[0] = 0
    for j = 1 to n
        q = -inf
        for i = 1 to j
            q = max(q, p[i] + r[j-i])
        r[j] = q
    return r[n]
#+END_SRC

- /subproblem  graph/: The subproblem graph has a directed edge from
  the vertex for subproblem =x= to the vertex for subproblem =y= if 
  determining an optimal solution for subproblem =x= involves directly
  considering an optimal solution for subproblem =y=.

The bottom-up method considers the vertices of the subproblem graph in
such an order that the subproblems =y= adjacent to a given subproblem
=x= is solved before solving subproblem =x=. No subproblem is considered
until all of the subproblems it depends upon have been solved.

The size of the subproblem graph $G=(V, E)$ can help us determine the
running time of the dynamic programming algorithm. The running time
is the sum of the times needed to solve each subproblem. The running
time of dynamic programming is linear in the number of vertices and 
edges.

#+BEGIN_SRC 
extended-bottom-up-cut-rod(p, n)  
    let r[0..n] and s[0..n] be new arrays
    r[0] = 0
    for j = 1 to n
        q = -inf
        for i = 1 to j
            if q < p[i] + r[j-i]
                q = p[i] + r[j-i]
                s[j] = i
        r[j] = q
    return r and s

#+END_SRC


*** Implementation

#+include: "./python-implementation/cut_rod.py" src python

** Elements of Dynamic Programming

Whenever a problem exhibits optimal substructure, dynamic programming might apply.
In dynamic programming, we build an optimal solution to the problem from optimal
solutions to subproblems.

To discover optimal substructure, there is a common pattern:

1. A solution to the /problem consists of making a choice/. Making this choice
  leaves one or more subproblems to be solved.

2. For a given problem, there is a choice that /leads to an optimal solution/.

3. Given this choice, determine which /subproblems ensue/ and how to best 
  characterize the resulting space of subproblems.

4. The solutions to the subproblems used within an optimal solution to the 
  problem must themselves be optimal by using a cut-and-paste technique: cutting
  out the nonoptimal solution and pasting in the optimal one, that is, a better
  solution is possible.

#+TODO: 

*** Longest Common Subsequence

Formally, given a sequence $X=\left\langle x_{1},x_{2},\dots,x_{m}\right\rangle$
another sequence $Z=\left\langle z_{1},z_{2},\dots,z_{k}\right\rangle$ is a 
subsequence of $X$ if there exists a strictly increasing sequence $\left\langle i_{1},i_{2},\dots,i_{k}\right\rangle$
of $X$ s.t. for all $j=1,2,\dots,k$ we have $x_{i_{j}} = z_j$.

/Longest common subsequence problem/: given two sequences $X$ and and $Y$, find
a maximum-length common subsequence of $X$ and $Y$.

**** Implementation

#+include: "./python-implementation/longest_common_sequence.py" src python

* Advanced Topics

#+TODO: 
